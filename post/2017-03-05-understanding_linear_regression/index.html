
<!DOCTYPE html>
<html
  lang="en"
  data-figures=""
  
    class="page"
  
  
  >
  <head>
<title>Understanding linear regression | Matt&#39;s Tech Blog</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">





<meta property="og:locale" content="en" />

<meta property="og:type" content="article">
<meta name="description" content="Regression represents one of the cornerstones of machine learning. Comprehending its logic and math provides a solid foundation for learning more advanced …" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:creator" content="@coming_soon">
<meta name="twitter:title" content="Understanding linear regression" />
<meta name="twitter:image" content="https://mmlind.github.io/images/thumbnail.png"/>
<meta property="og:url" content="https://mmlind.github.io/post/2017-03-05-understanding_linear_regression/" />
<meta property="og:title" content="Understanding linear regression" />
<meta property="og:description" content="Regression represents one of the cornerstones of machine learning. Comprehending its logic and math provides a solid foundation for learning more advanced …" />
<meta property="og:image" content="https://mmlind.github.io/images/thumbnail.png" />
  <meta name="keywords" content="ai,machine learning,nlp,nlm,large language models" />

<link rel="apple-touch-icon" sizes="180x180" href="https://mmlind.github.io/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://mmlind.github.io/icons/favicon-32x32.png">
<link rel="manifest" href="https://mmlind.github.io/icons/site.webmanifest">

<link rel="canonical" href="https://mmlind.github.io/post/2017-03-05-understanding_linear_regression/">



<link rel="preload" href="https://mmlind.github.io/css/styles.5bbdaff74df43ad3301fd1da44a332ca19afbd2dbb8c4f1e6c7fb5da394229112d9972011fedf21ba2cbd401586281122a5901efade33dec7837cceff22fa6ad.css" integrity = "sha512-W72v9030OtMwH9HaRKMyyhmvvS27jE8ebH&#43;12jlCKREtmXIBH&#43;3yG6LL1AFYYoESKlkB763jPex4N8zv8i&#43;mrQ==" as="style" crossorigin="anonymous">



<link rel="preload" href="https://mmlind.github.io/en/js/bundle.f4da32c64ece1e7d5a836039ceed09b7e4f3592da2a593a2c0e74dd8b24aef5d873eea6fcf180b171a60c193c271d3f845628a40d93531f8e49a553ed23162cd.js" as="script" integrity=
"sha512-9Noyxk7OHn1ag2A5zu0Jt&#43;TzWS2ipZOiwOdN2LJK712HPupvzxgLFxpgwZPCcdP4RWKKQNk1MfjkmlU&#43;0jFizQ==" crossorigin="anonymous">


<link rel="stylesheet" type="text/css" href="https://mmlind.github.io/css/styles.5bbdaff74df43ad3301fd1da44a332ca19afbd2dbb8c4f1e6c7fb5da394229112d9972011fedf21ba2cbd401586281122a5901efade33dec7837cceff22fa6ad.css" integrity="sha512-W72v9030OtMwH9HaRKMyyhmvvS27jE8ebH&#43;12jlCKREtmXIBH&#43;3yG6LL1AFYYoESKlkB763jPex4N8zv8i&#43;mrQ==" crossorigin="anonymous">

  </head>
  <body
    data-code="7"
    data-lines="false"
    id="documentTop"
    data-lang="en"
  >

<header class="nav_header" >
  <nav class="nav"><a href='https://mmlind.github.io/' class="nav_brand nav_item" title="Matt&#39;s Tech Blog">
  <img src="https://mmlind.github.io/logos/logo.png" class="logo" alt="Matt&#39;s Tech Blog">
  <div class="nav_close">
    <div><svg class="icon">
  <title>open-menu</title>
  <use xlink:href="#open-menu"></use>
</svg>
<svg class="icon">
  <title>closeme</title>
  <use xlink:href="#closeme"></use>
</svg>
</div>
  </div>
</a>

    <div class='nav_body nav_body_left'>
      
      
      
        

  <div class="nav_parent">
    <a href="https://mmlind.github.io/" class="nav_item" title="Blog">Blog </a>
  </div>
  <div class="nav_parent">
    <a href="https://mmlind.github.io/about/" class="nav_item" title="About">About </a>
  </div>
      
      <div class="nav_parent">
        <a href="#" class="nav_item"></a>
        <div class="nav_sub">
          <span class="nav_child"></span>
          
          <a href="https://mmlind.github.io/" class="nav_child nav_item">English</a>
          
          <a href="https://mmlind.github.io/de/" class="nav_child nav_item">Deutsch</a>
          
        </div>
      </div>
<div class='follow'>
  <a href="https://github.com/#">
    <svg class="icon">
  <title>github</title>
  <use xlink:href="#github"></use>
</svg>

  </a>
  <a href="https://cn.linkedin.com/in/mmlind">
    <svg class="icon">
  <title>linkedin</title>
  <use xlink:href="#linkedin"></use>
</svg>

  </a>
<div class="color_mode">
  <input type="checkbox" class="color_choice" id="mode">
</div>

</div>

    </div>
  </nav>
</header>

    <main>
  
<div class="grid-inverse wrap content">
  <article class="post_content">
    <h1 class="post_title">Understanding linear regression</h1>
  <div class="post_meta">
    <span><svg class="icon">
  <title>calendar</title>
  <use xlink:href="#calendar"></use>
</svg>
</span>
    <span class="post_date">
      Mar 5, 2017</span>
    <span class="post_time"> · 10 min read</span><span>&nbsp;· <a href='https://mmlind.github.io/tags/machine-learning/' title="MACHINE LEARNING" class="post_tag button button_translucent">MACHINE LEARNING
        </a><a href='https://mmlind.github.io/tags/math/' title="MATH" class="post_tag button button_translucent">MATH
        </a>
    </span>
    <span class="page_only">&nbsp;·
  <div class="post_share">
    Share on:
    <a href="https://twitter.com/intent/tweet?text=Understanding%20linear%20regression&url=https%3a%2f%2fmmlind.github.io%2fpost%2f2017-03-05-understanding_linear_regression%2f&tw_p=tweetbutton" class="twitter" title="Share on Twitter" target="_blank" rel="nofollow">
      <svg class="icon">
  <title>twitter</title>
  <use xlink:href="#twitter"></use>
</svg>

    </a>
    <a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fmmlind.github.io%2fpost%2f2017-03-05-understanding_linear_regression%2f&t=Understanding%20linear%20regression" class="facebook" title="Share on Facebook" target="_blank" rel="nofollow">
      <svg class="icon">
  <title>facebook</title>
  <use xlink:href="#facebook"></use>
</svg>

    </a>
    <a href="#linkedinshare" id = "linkedinshare" class="linkedin" title="Share on LinkedIn" rel="nofollow">
      <svg class="icon">
  <title>linkedin</title>
  <use xlink:href="#linkedin"></use>
</svg>

    </a>
    <a href="https://mmlind.github.io/post/2017-03-05-understanding_linear_regression/" title="Copy Link" class="link link_yank">
      <svg class="icon">
  <title>copy</title>
  <use xlink:href="#copy"></use>
</svg>

    </a>
  </div>
  </span>
  </div>

    <div class="post_body"><p>Regression represents one of the cornerstones of machine learning. Comprehending its logic and math provides a solid foundation for learning more advanced machine learning techniques such as neural networks.</p>
<p><figure>
  <picture>

    
      
        
        
        
        
        
        
    <img
      loading="lazy"
      decoding="async"
      alt=""
      
        class="image_figure image_internal image_unprocessed"
        src="linreg_blackboard.jpg"
      
      
    />

    </picture>
</figure>
</p>
<p>Linear regression is a statistical method for modeling the relationship between variables. It’s used for analyzing dependencies and predicting values.</p>
<p>The underlying idea of linear regression is simple: given a dataset, we want to find the dependency of one of the variables in the dataset on one, some or all of the other variables.</p>
<p>If we notate the dependent or target variable as <em>y</em> and the independent or explanatory variables as <em>x</em> what we’re looking for is the function</p>
<p>$$ y = f(x) $$</p>
<p>that, given an arbitrary value for <em>x</em>, outputs the desired target variable <em>y</em>. The output represents our <strong>prediction</strong> for <em>y</em>. It’s an estimation of the unknown actual value of <em>y</em>.</p>
<p>This function is also referred to as our <strong>model</strong>. We’re modeling the relationship between  and  or, more specifically, the dependency of <em>y</em> on <em>x</em>.</p>
<p>The dependency is learned from a <strong>dataset</strong>. The dataset is simply a large collection of examples of <em>x</em> recorded in the real world. These examples are used to <em>train</em> the computer to estimate the target function and hence called training examples.</p>
<p>Each training example may consist of just one or many explanatory variables  plus their respective <em>“to-be-predicted”</em> target variable <em>y</em>. It’s typically a many-to-one relationship, i.e. many independent variables are mapped to a single dependent variable.</p>
<p>The objective of linear regression is to have the computer learn dependencies inside the data simply from looking at many examples. Each example must include the <em>correct</em> or desired output for the respective inputs, also referred to as the <em>ground truth</em>.</p>
<p>This type of learning is therefore called <strong>supervised learning</strong>. By providing correct examples of <em>y</em> we’re <em>supervising</em> the learning algorithm.</p>
<h2 id="the-methodology">The Methodology</h2>
<p>Linear regression, or regression analysis in general, consists of 3 steps: define a hypothesis function, define a cost function and run an optimization algorithm.</p>
<p>Before explaining these components in detail, let me briefly address a hurdle that many people face when learning regression or machine learning in general: the math.</p>
<p>If you don’t have a strong math background a series of equations presented in a tutorial or paper may present an immediate turn-off. I know. Why is that so?</p>
<p>Math is a language, <strong>the language of nature</strong>. Unfortunately, the average reader is not fluent in or familiar with this language. Mathematical equations therefore easily get intimidating because they may include unknown signs or greek letters.</p>
<p>Let’s go step by step through the math, and you’ll see it’s not that hard at all. Especially if you’re a software developer and used to systematic thinking.</p>
<h3 id="mathematical-notation">Mathematical Notation</h3>
<p>A dataset consists of a number of training samples. Typically, the more the better. Let’s define or notate the size of the dataset as <em>m</em>:</p>
<p>$$m = \text{number of training examples in the dataset} $$</p>
<p>Every training sample consists of a number of independent or explanatory values, called <strong>features</strong>. Let’s notate the number of such features per training example as <em>n</em>:</p>
<p>$$n = \text{number of features per training example} $$</p>
<p>Then let’s pack all of these features of a single training example into a vector. In this context a vector can be regarded simply as a list, or array, of values. Let’s notate this vector as <em>x</em>:</p>
<p>$$ x = [x_1, x_2, ..., x_n] $$</p>
<p>The subscript index denotes the <em>j-th</em> feature of a training example, where</p>
<p>$$ j \in \lbrace 1..n \rbrace $$</p>
<p>But of course there are many training examples, not just a single one. Therefore, we will use another index, a superscript <em>i</em>, to denote a specific training example <em>i</em>, where</p>
<p>$$ i \in \lbrace 1..m \rbrace $$</p>
<p>A specific training example, with all its features, is then denoted as vector</p>
<p>$$ x^{(i)} = \text{n-dimensional vector of all features of the i-th training sample} $$</p>
<p>Combining both, the value of a specific feature of a specific training example is denoted as:</p>
<p>$$ x_j^{(i)} = \text{value of the } j^{th} \text{ feature in the } i^{th} \text{ training sample } $$</p>
<p>So far so good. That wasn’t so hard.</p>
<p>Now, what’s missing? To compute, or estimate, the target variable linear regression <em>weighs</em> each individual feature differently and then adds up all the weighted features.</p>
<p>These weights are called the <strong>parameters</strong> and are notated with the greek letter “Theta”:</p>
<p>$$ \theta $$</p>
<p>Every feature gets weighted differently. The number of parameters that we’re trying to find is therefore the same as the number of features.</p>
<p>But we don’t compute separate parameters for each training example. What we want is a <strong>single</strong> model, one function that covers <strong>all</strong> of the training examples. So let’s pack the parameters into a vector as well and denote this vector as:</p>
<p>$$ \theta = [\theta_1, \theta_2, ..., \theta_n] $$</p>
<p>Great. That’s all we need. Now, let’s get started with the 3 steps methodology mentioned above.</p>
<p><figure>
  <picture>

    
      
        
        
        
        
        
        
    <img
      loading="lazy"
      decoding="async"
      alt=""
      
        class="image_figure image_internal image_unprocessed"
        src="linreg_hypothesis.jpg"
      
      
    />

    </picture>
</figure>
</p>
<h3 id="hypothesis-function">Hypothesis Function</h3>
<p>Regression attempts to model an unknown function <em>h</em> that maps the given inputs <em>x</em> to an output value <em>y</em>. Let’s call this our <strong>hypothesis function</strong> and denote it as</p>
<p>$$ h = f(x) $$</p>
<p>Again, there could be a single input value <em>x</em> or many input values. (Remember, <em>x</em> is a vector and this vector may have a size of 1 which would make it a scalar.)</p>
<p>Depending on the number of explanatory variables <em>x</em>, we distinguish 3 types of linear regression:</p>
<ul>
<li>If there is only a <strong>single</strong> input variable or feature <em>x</em>, we call the model a <strong>uni-variate</strong> linear regression:</li>
</ul>
<p>$$ h_\theta(x) = \theta_0 + \theta_1 x $$</p>
<ul>
<li>If there are <strong>multiple</strong> input variables or features <em>x</em>, we call the model a <strong>multi-variate</strong> linear regression:</li>
</ul>
<p>$$ h_\theta(x) = \theta_0 + \theta_1 x_1 + ...\ \theta_i x_i \ ... + \theta_n x_n $$</p>
<ul>
<li>If the independent variables or features include <strong>exponentials</strong>, we call the model a <strong>polynomial</strong> linear regression:</li>
</ul>
<p>$$ h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2 + \theta_3 x_1 x_2 + \theta_4 x_2^2 \ ... $$</p>
<p>Math and methodology are the same for all three.</p>
<p>You may have noticed that I added an additional parameter</p>
<p>$$ \theta_0 $$</p>
<p>The reason is that the function that we’re trying to find may have a certain <strong>base</strong> value that is completely independent from any of the features. This base is also called the <strong>bias</strong>.</p>
<p>Since there is no feature mapped or related to the bias, we set the corresponding</p>
<p>$$ x_0 = 1 $$</p>
<p>Having added the bias parameter to our feature vector <em>x</em> and to our parameter vector <em>0</em>, both vectors become of size <em>n+1</em>.</p>
<p><figure>
  <picture>

    
      
        
        
        
        
        
        
    <img
      loading="lazy"
      decoding="async"
      alt=""
      
        class="image_figure image_internal image_unprocessed"
        src="linreg_costfunction.jpg"
      
      
    />

    </picture>
</figure>
</p>
<h3 id="cost-function">Cost Function</h3>
<p>After we defined the hypothesis function, we can feed data into this function and it will return a prediction or an estimate of what the output value <em>y</em> should be for a particular set of inputs. But how good is this value computed via our hypothesis?</p>
<p>We need to measure the <strong>accuracy</strong> of our hypothesis. We do so by computing the difference of the hypothesis with the actual <em>“true”</em> target value from the dataset. (Remember from above that in supervised learning we need to provide the target value for each training example.)</p>
<p>Since there are many training examples and not just a single one, we can measure the difference for each. That’s what a cost function does.</p>
<p>The difference of hypothesis and target is called the <strong>error</strong> or <strong>cost</strong> or <strong>loss</strong>.</p>
<p>Conceptually, there are many different options how to measure or express this error. The most common method is to compute the <strong>mean squared error (MSE)</strong> and is defined as follows:</p>
<ul>
<li>
<p>compute the difference of hypothesis and correct output for a particular training example <em>i</em>,</p>
</li>
<li>
<p>square the difference,</p>
</li>
<li>
<p>sum the squared differences across all <em>m</em> training examples, and then</p>
</li>
<li>
<p>divide the total by <em>m</em> to get the mean (average) cost.</p>
</li>
</ul>
<p>Mathematically, we’ll denote this kind of cost function <em>J</em> as</p>
<p>$$ J (\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}  \sum\limits_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)}  )^2 $$</p>
<p>and read it as <em><strong>“J with respect to Theta 1, ... Theta n”</strong></em> stating in brackets the variable(s) that we’re minimizing for (i.e. Theta).</p>
<p>Next, we want to <strong>minimize</strong> this cost function, i.e. find those parameters or those values of <em>0</em> that result in the minimum <em>J</em>.</p>
<p>Minimizing the cost is an <strong>optimization problem</strong>: we want to find the optimal representation of</p>
<p>$$ h(x) $$</p>
<p>which leads us to the third and last step in the methodology for linear regression.</p>
<p>Before we do that, a short side note. You probably noticed that in above formula for <em>J</em> we’re actually dividing by <em>2m</em> instead of simply by <em>m</em>. This is done for mathematical convenience later (when computing the derivate). Since it doesn’t impact the minimization it represents a valid simplification.</p>
<p>At this point it may also be worth noting that in addition to mean squared error there are other possible cost functions, e.g. one could compute the mean <strong>absolute</strong> error or the <strong>average cross entropy</strong>.</p>
<p><figure>
  <picture>

    
      
        
        
        
        
        
        
    <img
      loading="lazy"
      decoding="async"
      alt=""
      
        class="image_figure image_internal image_unprocessed"
        src="linreg_optimization.jpg"
      
      
    />

    </picture>
</figure>
</p>
<h3 id="optimization-algorithm">Optimization Algorithm</h3>
<p>To find the optimal hypothesis we need to minimize the error or cost. The optimal values for our parameters Theta will result in the lowest cost or lowest value for <em>J</em>.</p>
<p>There are different ways to do this. The most popular optimization algorithm is called gradient descent.</p>
<h3 id="gradient-descent">Gradient Descent</h3>
<p>The idea of gradient descent is simple:</p>
<ul>
<li>
<p>Compute the slope at an arbitrary point of the cost curve.</p>
</li>
<li>
<p>If the slope is <strong>bigger</strong> than 0, slightly <strong>decrease</strong> the input values, i.e. pick a new point a little further to the <strong>left</strong> on the cost curve.</p>
</li>
<li>
<p>If the slope is <strong>smaller</strong> than 0, we slightly **
** our input values, i.e. pick a new point a little further to the <strong>right</strong> on the cost curve.</p>
</li>
<li>
<p>Repeat these steps until the slope is so small that there is hardly any further reduction or improvement.</p>
</li>
</ul>
<p>Let’s see how to do this mathematically.</p>
<p>The slope of a curve is computed via the derivative of the underlying function. That’s easy. But our function does not have one but <strong>multiple</strong> features and the same number of parameters. Hence we need to compute the <strong>partial derivate</strong> for <strong>each</strong> of those parameters.</p>
<p>Gradient descent incrementally updates the parameters in our cost function using two components: the <strong>gradient</strong> and a <strong>step size</strong>.</p>
<p>The partial derivative (<em><strong>“delta”</strong></em>) of the cost function <em>J</em></p>
<p>$$ J (\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}  \sum\limits_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)}  )^2 $$</p>
<p>with respect to parameter</p>
<p>$$ \theta_j $$</p>
<p>is called the <strong>gradient</strong>:</p>
<p>$$ \frac{\delta}{\delta \theta_j} J (\theta_0, \theta_1, ..., \theta_n) = \frac{1}{m}  \sum\limits_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)}  ) \ x_j^{(i)} $$</p>
<p>This gradient is used to update or change the parameters. It will be subtracted at an arbitrary “step size” called the <strong>learning rate</strong> <em>alpha</em>:</p>
<p>$$ \alpha $$</p>
<p>So, we simultaneously update all parameters</p>
<p>$$ \theta_j \text{ for } j \in \lbrace 0..n \rbrace $$</p>
<p>via the following formula:</p>
<p>$$ \theta_j := \theta_j - \alpha \frac{\delta}{\delta \theta_j} J (\theta) = \theta_j - \alpha \frac{1}{m}  \sum\limits_{i=1}^m (h_{\theta} (x^{(i)}) - y^{(i)}  ) \ x_j^{(i)} $$</p>
<p>Gradient descent automatically takes smaller steps when it approaches the minimum because the slope gets flatter.</p>
<p>There are 3 types of gradient descent:</p>
<ul>
<li>
<p><strong>batch</strong> gradient descent = each step uses all training samples (=&gt; computationally expensive)</p>
</li>
<li>
<p><strong>stochastic</strong> gradient descent = each step uses only a single sample</p>
</li>
<li>
<p><strong>mini batch</strong> gradient descent = each step uses an arbitrary mini-batch size of training samples</p>
</li>
</ul>
<h4 id="batch-gradient-descent">Batch Gradient Descent</h4>
<p>Batch Gradient Descent considers all training examples for each optimization step. It directly converges to the global minimum.</p>
<h4 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h4>
<p>Stochastic gradient descent considers only a single training example at a time, therefore does not directly converge but zig-zags in on the global minimum. It does not actually reach the global minimum, but “oscillates” around it.</p>
<p>Looping through the whole dataset only once may not be sufficient to converge to a global optimum. In practice SGD is often run 1-10 times on a given dataset.</p>
<p>SGD is a type of <strong>online learning</strong> algorithm because it does not require a whole dataset at once but can process training examples one-by-one, i.e. learn <em>online</em>.</p>
<p><figure>
  <picture>

    
      
        
        
        
        
        
        
    <img
      loading="lazy"
      decoding="async"
      alt=""
      
        class="image_figure image_internal image_unprocessed"
        src="linreg_bigdata.jpg"
      
      
    />

    </picture>
</figure>
</p>
<h4 id="mini-batch-gradient-descent">Mini Batch Gradient Descent</h4>
<p>Mini Batch Gradient Descent is a kind of hybrid version trying to combine the advantages of both batch and stochastic gradient descent. Instead of summing (or looping through) the whole dataset for each optimization step, the algorithm uses a <strong>randomly selected subset</strong> of examples.</p>
<p>Its main advantage is that it’s computationally much less expensive. It also converges faster than stochastic gradient descent because it avoids (much of) the zig-zag.</p>
<p>Given today’s parallel computing capabilities mini-batch is especially performant if the algorithm is vectorized.</p>
<p>Thanks for reading! Feel free to contact me for any questions or comments.</p>

    </div>
<div class="post_comments">
  
  
    
 <script src="https://utteranc.es/client.js"
         repo="https://github.com/mmlind/mmlind.github.io"
         issue-term="pathname"
         theme="github-dark"
         
         label="blog comments ✨💬✨"
         
         crossorigin="anonymous"
         async>
 </script>
 
  
  
</div>




  </article>
<aside class="sidebar">
  <section class="sidebar_inner">
    <br>
    
  
  <div class="search">
    <input type="search" class="search_field form_field" placeholder='Search...' id="find" autocomplete="off" data-scope='post'>
    <label for="find" class="search_label"><svg class="icon">
  <title>search</title>
  <use xlink:href="#search"></use>
</svg>

    </label>
    
    <div class="search_results results"></div>
  </div>

        <h2>Matt Lind</h2>
      <div class="author_bio">
        Tech geek, command-line aficionado, AI enthusiast, life-long learner.
      </div>
      <a href='https://mmlind.github.io/about/' class="button mt-1" role="button" title='Read More'>Read More</a>

    
    
    <h2 class="mt-4">Featured Posts</h2>
    <ul>
      <li>
        <a href="https://mmlind.github.io/post/2023-05-14-how_to_make_chatgpt_work_with_your_own_data/" class="nav-link" title="How to make ChatGPT work with your own data">How to make ChatGPT work with your own data</a>
      </li>
    </ul>
    <h2 class="mt-4">Recent Posts</h2>
    <ul class="flex-column">
      <li>
        <a href="https://mmlind.github.io/post/2020-10-16-reading_all_of_wikipedia_in_6_seconds_how_to_utilize_multiple_cores_to_process_very_large_text_files/" class="nav-link" title="Reading all of Wikipedia in 6 seconds: how to utilize multiple cores to process very large text files">Reading all of Wikipedia in 6 seconds: how to utilize multiple cores to process very large text files</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/post/2020-10-05-how_to_simultaneously_write_to_shared_memory_with_multiple_processes/" class="nav-link" title="How to simultaneously write to shared memory with multiple processes">How to simultaneously write to shared memory with multiple processes</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/post/2020-09-29-migrating_my_github-pages_blog_from_jekyl_to_hugo/" class="nav-link" title="Migrating my GitHub pages blog from Jekyl to Hugo">Migrating my GitHub pages blog from Jekyl to Hugo</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/post/2017-12-26-using_logistic_regression_to_classify_images/" class="nav-link" title="Using logistic regression to classify images">Using logistic regression to classify images</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/post/2017-03-05-understanding_linear_regression/" class="nav-link" title="Understanding linear regression">Understanding linear regression</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/post/2016-02-12-deep_neural_network_for_mnist_handwriting_recognition/" class="nav-link" title="Deep Neural Network for MNIST Handwriting Recognition">Deep Neural Network for MNIST Handwriting Recognition</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/post/2015-08-09-simple_3-layer_neural_network_for_mnist_handwriting_recognition/" class="nav-link" title="Simple 3-Layer Neural Network for MNIST Handwriting Recognition">Simple 3-Layer Neural Network for MNIST Handwriting Recognition</a>
      </li>
    </ul>
    <div>
      <h2 class="mt-4 taxonomy" id="tags-section">Tags</h2>
      <nav class="tags_nav">
        <a href='https://mmlind.github.io/tags/machine-learning/' class="post_tag button button_translucent" title="machine-learning">
          MACHINE-LEARNING
          <span class="button_tally">6</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/computer-vision/' class="post_tag button button_translucent" title="computer-vision">
          COMPUTER-VISION
          <span class="button_tally">4</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/blogging/' class="post_tag button button_translucent" title="blogging">
          BLOGGING
          <span class="button_tally">2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/c/' class="post_tag button button_translucent" title="c">
          C
          <span class="button_tally">2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/math/' class="post_tag button button_translucent" title="math">
          MATH
          <span class="button_tally">2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/multiprocessing/' class="post_tag button button_translucent" title="multiprocessing">
          MULTIPROCESSING
          <span class="button_tally">2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/natural-language-processing/' class="post_tag button button_translucent" title="natural-language-processing">
          NATURAL-LANGUAGE-PROCESSING
          <span class="button_tally">2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/nlp/' class="post_tag button button_translucent" title="nlp">
          NLP
          <span class="button_tally">2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/python/' class="post_tag button button_translucent" title="python">
          PYTHON
          <span class="button_tally">2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/chat-gpt/' class="post_tag button button_translucent" title="chat-gpt">
          CHAT-GPT
          <span class="button_tally">1</span>
        </a>
        
        
      </nav>
    </div>
  </section>
</aside>

  
</div>
    </main><svg width="0" height="0" class="hidden">
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="facebook">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h151V331h-60v-90h60v-61c0-49.629 40.371-90 90-90h91v90h-91v61h91l-15 90h-76v181h121c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18.001 18.001" id="twitter">
    <path d="M15.891 4.013c.808-.496 1.343-1.173 1.605-2.034a8.68 8.68 0 0 1-2.351.861c-.703-.756-1.593-1.14-2.66-1.14-1.043 0-1.924.366-2.643 1.078a3.56 3.56 0 0 0-1.076 2.605c0 .309.039.585.117.819-3.076-.105-5.622-1.381-7.628-3.837-.34.601-.51 1.213-.51 1.846 0 1.301.549 2.332 1.645 3.089-.625-.053-1.176-.211-1.645-.47 0 .929.273 1.705.82 2.388a3.623 3.623 0 0 0 2.115 1.291c-.312.08-.641.118-.979.118-.312 0-.533-.026-.664-.083.23.757.664 1.371 1.291 1.841a3.652 3.652 0 0 0 2.152.743C4.148 14.173 2.625 14.69.902 14.69c-.422 0-.721-.006-.902-.038 1.697 1.102 3.586 1.649 5.676 1.649 2.139 0 4.029-.542 5.674-1.626 1.645-1.078 2.859-2.408 3.639-3.974a10.77 10.77 0 0 0 1.172-4.892v-.468a7.788 7.788 0 0 0 1.84-1.921 8.142 8.142 0 0 1-2.11.593z"
      ></path>
  </symbol>
  <symbol aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="mail">
    <path  d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="calendar">
    <path d="M452 40h-24V0h-40v40H124V0H84v40H60C26.916 40 0 66.916 0 100v352c0 33.084 26.916 60 60 60h392c33.084 0 60-26.916 60-60V100c0-33.084-26.916-60-60-60zm20 412c0 11.028-8.972 20-20 20H60c-11.028 0-20-8.972-20-20V188h432v264zm0-304H40v-48c0-11.028 8.972-20 20-20h24v40h40V80h264v40h40V80h24c11.028 0 20 8.972 20 20v48z"></path>
    <path d="M76 230h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 310h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 390h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80-80h40v40h-40z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="github">
    <path d="M255.968 5.329C114.624 5.329 0 120.401 0 262.353c0 113.536 73.344 209.856 175.104 243.872 12.8 2.368 17.472-5.568 17.472-12.384 0-6.112-.224-22.272-.352-43.712-71.2 15.52-86.24-34.464-86.24-34.464-11.616-29.696-28.416-37.6-28.416-37.6-23.264-15.936 1.728-15.616 1.728-15.616 25.696 1.824 39.2 26.496 39.2 26.496 22.848 39.264 59.936 27.936 74.528 21.344 2.304-16.608 8.928-27.936 16.256-34.368-56.832-6.496-116.608-28.544-116.608-127.008 0-28.064 9.984-51.008 26.368-68.992-2.656-6.496-11.424-32.64 2.496-68 0 0 21.504-6.912 70.4 26.336 20.416-5.696 42.304-8.544 64.096-8.64 21.728.128 43.648 2.944 64.096 8.672 48.864-33.248 70.336-26.336 70.336-26.336 13.952 35.392 5.184 61.504 2.56 68 16.416 17.984 26.304 40.928 26.304 68.992 0 98.72-59.84 120.448-116.864 126.816 9.184 7.936 17.376 23.616 17.376 47.584 0 34.368-.32 62.08-.32 70.496 0 6.88 4.608 14.88 17.6 12.352C438.72 472.145 512 375.857 512 262.353 512 120.401 397.376 5.329 255.968 5.329z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 212 212" id="gitlab">
    <path d="M12.3 74.7h54L43.3 3c-1-3.6-6.4-3.6-7.6 0L12.3 74.8z" />
    <path d="M12.3 74.7L.5 111c-1 3.2 0 6.8 3 8.8l101.6 74-92.5-119z"/>
    <path d="M105 193.7l-38.6-119h-54l92.7 119z"/>
    <path d="M105 193.7l38.7-119H66.4l38.7 119z"/>
    <path d="M105 193.7l38.7-119H198l-93 119z"/>
    <path d="M198 74.7l11.6 36.2c1 3 0 6.6-3 8.6l-101.5 74 93-119z"/>
    <path d="M198 74.7h-54.3L167 3c1.2-3.6 6.4-3.6 7.6 0L198 74.8z"/>
  </symbol>
  <symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="rss">
    <circle cx="3.429" cy="20.571" r="3.429"></circle>
    <path d="M11.429 24h4.57C15.999 15.179 8.821 8.001 0 8v4.572c6.302.001 11.429 5.126 11.429 11.428z"></path>
    <path d="M24 24C24 10.766 13.234 0 0 0v4.571c10.714 0 19.43 8.714 19.43 19.429z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="linkedin">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h362c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zM181 406h-60V196h60zm0-240h-60v-60h60zm210 240h-60V286c0-16.54-13.46-30-30-30s-30 13.46-30 30v120h-60V196h60v11.309C286.719 202.422 296.93 196 316 196c40.691.043 75 36.547 75 79.688zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 612 612" id="to-top">
    <path d="M604.501 440.509L325.398 134.956c-5.331-5.357-12.423-7.627-19.386-7.27-6.989-.357-14.056 1.913-19.387 7.27L7.499 440.509c-9.999 10.024-9.999 26.298 0 36.323s26.223 10.024 36.222 0l262.293-287.164L568.28 476.832c9.999 10.024 26.222 10.024 36.221 0 9.999-10.023 9.999-26.298 0-36.323z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="carly">
    <path d="M504.971 239.029L448 182.059V84c0-46.317-37.682-84-84-84h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c19.851 0 36 16.149 36 36v108c0 6.365 2.529 12.47 7.029 16.971L454.059 256l-47.029 47.029A24.002 24.002 0 0 0 400 320v108c0 19.851-16.149 36-36 36h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c46.318 0 84-37.683 84-84v-98.059l56.971-56.971c9.372-9.372 9.372-24.568 0-33.941zM112 192V84c0-19.851 16.149-36 36-36h44c13.255 0 24-10.745 24-24S205.255 0 192 0h-44c-46.318 0-84 37.683-84 84v98.059l-56.971 56.97c-9.373 9.373-9.373 24.568 0 33.941L64 329.941V428c0 46.317 37.682 84 84 84h44c13.255 0 24-10.745 24-24s-10.745-24-24-24h-44c-19.851 0-36-16.149-36-36V320c0-6.365-2.529-12.47-7.029-16.971L57.941 256l47.029-47.029A24.002 24.002 0 0 0 112 192z"></path>
  </symbol>
  <symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="copy">
    <path d="M23 2.75A2.75 2.75 0 0 0 20.25 0H8.75A2.75 2.75 0 0 0 6 2.75v13.5A2.75 2.75 0 0 0 8.75 19h11.5A2.75 2.75 0 0 0 23 16.25zM18.25 14.5h-7.5a.75.75 0 0 1 0-1.5h7.5a.75.75 0 0 1 0 1.5zm0-3h-7.5a.75.75 0 0 1 0-1.5h7.5a.75.75 0 0 1 0 1.5zm0-3h-7.5a.75.75 0 0 1 0-1.5h7.5a.75.75 0 0 1 0 1.5z"></path>
    <path d="M8.75 20.5a4.255 4.255 0 0 1-4.25-4.25V2.75c0-.086.02-.166.025-.25H3.75A2.752 2.752 0 0 0 1 5.25v16A2.752 2.752 0 0 0 3.75 24h12a2.752 2.752 0 0 0 2.75-2.75v-.75z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512.001 512.001" id="closeme">
    <path d="M284.286 256.002L506.143 34.144c7.811-7.811 7.811-20.475 0-28.285-7.811-7.81-20.475-7.811-28.285 0L256 227.717 34.143 5.859c-7.811-7.811-20.475-7.811-28.285 0-7.81 7.811-7.811 20.475 0 28.285l221.857 221.857L5.858 477.859c-7.811 7.811-7.811 20.475 0 28.285a19.938 19.938 0 0 0 14.143 5.857 19.94 19.94 0 0 0 14.143-5.857L256 284.287l221.857 221.857c3.905 3.905 9.024 5.857 14.143 5.857s10.237-1.952 14.143-5.857c7.811-7.811 7.811-20.475 0-28.285L284.286 256.002z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="open-menu">
    <path d="M492 236H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0-160H20C8.954 76 0 84.954 0 96s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0 320H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="instagram">
    <path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id=youtube>
    <path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="stackoverflow">
    <path d="M21 27v-8h3v11H0V19h3v8h18z"></path><path d="M17.1.2L15 1.8l7.9 10.6 2.1-1.6L17.1.2zm3.7 14.7L10.6 6.4l1.7-2 10.2 8.5-1.7 2zM7.2 12.3l12 5.6 1.1-2.4-12-5.6-1.1 2.4zm-1.8 6.8l13.56 1.96.17-2.38-13.26-2.55-.47 2.97zM19 25H5v-3h14v3z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="xing">
    <path d="M18.188 0c-.517 0-.741.325-.927.66 0 0-7.455 13.224-7.702 13.657.015.024 4.919 9.023 4.919 9.023.17.308.436.66.967.66h3.454c.211 0 .375-.078.463-.22.089-.151.089-.346-.009-.536l-4.879-8.916c-.004-.006-.004-.016 0-.022L22.139.756c.095-.191.097-.387.006-.535C22.056.078 21.894 0 21.686 0h-3.498zM3.648 4.74c-.211 0-.385.074-.473.216-.09.149-.078.339.02.531l2.34 4.05c.004.01.004.016 0 .021L1.86 16.051c-.099.188-.093.381 0 .529.085.142.239.234.45.234h3.461c.518 0 .766-.348.945-.667l3.734-6.609-2.378-4.155c-.172-.315-.434-.659-.962-.659H3.648v.016z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 71 55" id="discord">
    <path d="M60.1045 4.8978C55.5792 2.8214 50.7265 1.2916 45.6527 0.41542C45.5603 0.39851 45.468 0.440769 45.4204 0.525289C44.7963 1.6353 44.105 3.0834 43.6209 4.2216C38.1637 3.4046 32.7345 3.4046 27.3892 4.2216C26.905 3.0581 26.1886 1.6353 25.5617 0.525289C25.5141 0.443589 25.4218 0.40133 25.3294 0.41542C20.2584 1.2888 15.4057 2.8186 10.8776 4.8978C10.8384 4.9147 10.8048 4.9429 10.7825 4.9795C1.57795 18.7309 -0.943561 32.1443 0.293408 45.3914C0.299005 45.4562 0.335386 45.5182 0.385761 45.5576C6.45866 50.0174 12.3413 52.7249 18.1147 54.5195C18.2071 54.5477 18.305 54.5139 18.3638 54.4378C19.7295 52.5728 20.9469 50.6063 21.9907 48.5383C22.0523 48.4172 21.9935 48.2735 21.8676 48.2256C19.9366 47.4931 18.0979 46.6 16.3292 45.5858C16.1893 45.5041 16.1781 45.304 16.3068 45.2082C16.679 44.9293 17.0513 44.6391 17.4067 44.3461C17.471 44.2926 17.5606 44.2813 17.6362 44.3151C29.2558 49.6202 41.8354 49.6202 53.3179 44.3151C53.3935 44.2785 53.4831 44.2898 53.5502 44.3433C53.9057 44.6363 54.2779 44.9293 54.6529 45.2082C54.7816 45.304 54.7732 45.5041 54.6333 45.5858C52.8646 46.6197 51.0259 47.4931 49.0921 48.2228C48.9662 48.2707 48.9102 48.4172 48.9718 48.5383C50.038 50.6034 51.2554 52.5699 52.5959 54.435C52.6519 54.5139 52.7526 54.5477 52.845 54.5195C58.6464 52.7249 64.529 50.0174 70.6019 45.5576C70.6551 45.5182 70.6887 45.459 70.6943 45.3942C72.1747 30.0791 68.2147 16.7757 60.1968 4.9823C60.1772 4.9429 60.1437 4.9147 60.1045 4.8978ZM23.7259 37.3253C20.2276 37.3253 17.3451 34.1136 17.3451 30.1693C17.3451 26.225 20.1717 23.0133 23.7259 23.0133C27.308 23.0133 30.1626 26.2532 30.1066 30.1693C30.1066 34.1136 27.28 37.3253 23.7259 37.3253ZM47.3178 37.3253C43.8196 37.3253 40.9371 34.1136 40.9371 30.1693C40.9371 26.225 43.7636 23.0133 47.3178 23.0133C50.9 23.0133 53.7545 26.2532 53.6986 30.1693C53.6986 34.1136 50.9 37.3253 47.3178 37.3253Z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17 18" id="mastodon">
    <path
    fill="#ffffff"
    d="m 15.054695,9.8859583 c -0.22611,1.1632697 -2.02517,2.4363497 -4.09138,2.6830797 -1.0774504,0.12856 -2.1382704,0.24673 -3.2694704,0.19484 -1.84996,-0.0848 -3.30971,-0.44157 -3.30971,-0.44157 0,0.1801 0.0111,0.35157 0.0333,0.51194 0.24051,1.82571 1.81034,1.93508 3.29737,1.98607 1.50088,0.0514 2.8373104,-0.37004 2.8373104,-0.37004 l 0.0617,1.35686 c 0,0 -1.0498104,0.56374 -2.9199404,0.66742 -1.03124,0.0567 -2.3117,-0.0259 -3.80308,-0.42069 -3.23454998,-0.85613 -3.79081998,-4.304 -3.87592998,-7.8024197 -0.026,-1.03871 -0.01,-2.01815 -0.01,-2.83732 0,-3.57732 2.34385998,-4.62587996 2.34385998,-4.62587996 1.18184,-0.54277 3.20976,-0.77101 5.318,-0.7882499985409 h 0.0518 C 9.8267646,0.01719834 11.856025,0.24547834 13.037775,0.78824834 c 0,0 2.34377,1.04855996 2.34377,4.62587996 0,0 0.0294,2.63937 -0.32687,4.47183"/>
 <path
    fill="#000000"
    d="m 12.616925,5.6916583 v 4.3315297 h -1.71607 V 5.8189683 c 0,-0.88624 -0.37289,-1.33607 -1.1187604,-1.33607 -0.82467,0 -1.23799,0.53361 -1.23799,1.58875 v 2.30122 h -1.70594 v -2.30122 c 0,-1.05514 -0.4134,-1.58875 -1.23808,-1.58875 -0.74587,0 -1.11876,0.44983 -1.11876,1.33607 v 4.2042197 h -1.71607 V 5.6916583 c 0,-0.88527 0.22541,-1.58876 0.67817,-2.10922 0.46689,-0.52047 1.07833,-0.78727 1.83735,-0.78727 0.87816,0 1.54317,0.33752 1.98288,1.01267 l 0.42744,0.71655 0.42753,-0.71655 c 0.43961,-0.67515 1.10463,-1.01267 1.9828704,-1.01267 0.75893,0 1.37037,0.2668 1.83735,0.78727 0.45268,0.52046 0.67808,1.22395 0.67808,2.10922"/>
  </symbol>
</svg>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

<footer class="footer">
  <div class="footer_inner wrap pale">
    <img src='https://mmlind.github.io/icons/cli.png' class="icon icon_2 transparent" alt="Matt&#39;s Tech Blog">
    <p>Copyright&nbsp;2015-&nbsp;<span class="year"></span>&nbsp;MATT&#39;S TECH BLOG. All Rights Reserved</p><a class="to_top" href="#documentTop">
  <svg class="icon">
  <title>to-top</title>
  <use xlink:href="#to-top"></use>
</svg>

</a>

  </div>
</footer>

<script type="text/javascript" src="https://mmlind.github.io/en/js/bundle.f4da32c64ece1e7d5a836039ceed09b7e4f3592da2a593a2c0e74dd8b24aef5d873eea6fcf180b171a60c193c271d3f845628a40d93531f8e49a553ed23162cd.js" integrity="sha512-9Noyxk7OHn1ag2A5zu0Jt&#43;TzWS2ipZOiwOdN2LJK712HPupvzxgLFxpgwZPCcdP4RWKKQNk1MfjkmlU&#43;0jFizQ==" crossorigin="anonymous"></script>

  <script src="https://mmlind.github.io/js/search.min.441534ebca8f29b72ee98c817c1d9c475fc24ae0a88f1c2eb4deacb203fccebce3c0eee3c758545c399671772a0bc025c7e45b2b1396a19a6dff7ead9c73f066.js"></script>

  </body>
</html>
