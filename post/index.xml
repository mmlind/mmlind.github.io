<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Matt&#39;s Tech Blog</title>
    <link>https://mmlind.github.io/post/</link>
    <description>Recent content in Posts on Matt&#39;s Tech Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 14 May 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://mmlind.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to setup your own ChatGPT and connect it to your own data</title>
      <link>https://mmlind.github.io/post/2023-05-14-how_to_setup_your_own_chatgpt_and_connect_it_to_your_own_data/</link>
      <pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2023-05-14-how_to_setup_your_own_chatgpt_and_connect_it_to_your_own_data/</guid>
      <description>
        
          
            TLDR: I&#39;m going through a simple POC (Proof of Concept) how companies can setup their own ChatGPT-like professional agents without sending their confidential data to OpenAI or to any other online service. After a general introduction to how LLMs are trained, I show how you can use Dolly (a publicly available and commercially usable fine-tuned language model) and Langchain (an open-source NLP library) to make your own data sources (e.g. from documents, databases, other systems) accessible via natural language using vector data stores and semantic search.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Reading all of Wikipedia in 6 seconds: how to utilize multiple cores to process very large text files</title>
      <link>https://mmlind.github.io/post/2020-10-16-reading_all_of_wikipedia_in_6_seconds_how_to_utilize_multiple_cores_to_process_very_large_text_files/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2020-10-16-reading_all_of_wikipedia_in_6_seconds_how_to_utilize_multiple_cores_to_process_very_large_text_files/</guid>
      <description>
        
          
            I was about to do some basic natural language processing (NLP) task on a large text file: create a dictionary of unique words and count how many times each word occurs. Easy peasy. This should only take me a minute. So I thought. While coding did not take more than 2 zips of coffee, I needed to wait almost an hour for the task to complete. That was unacceptable. There must be a way to accelerate this process.
          
          
        
      </description>
    </item>
    
    <item>
      <title>How to simultaneously write to shared memory with multiple processes</title>
      <link>https://mmlind.github.io/post/2020-10-05-how_to_simultaneously_write_to_shared_memory_with_multiple_processes/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2020-10-05-how_to_simultaneously_write_to_shared_memory_with_multiple_processes/</guid>
      <description>
        
          
            Most of our modern day computers have more than one processing unit. They are often referred to as cores. For a developer, effectively using multiple cores simultaneously to speed up program execution by parallel processing comes with challenges. One of them is changing the value of a variable in shared memory.
In this blog post I introduce one of the two most common methods to overcome this problem by synchronizing access to shared memory: fork and semaphores.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Migrating my GitHub pages blog from Jekyl to Hugo</title>
      <link>https://mmlind.github.io/post/2020-09-29-migrating_my_github-pages_blog_from_jekyl_to_hugo/</link>
      <pubDate>Tue, 29 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2020-09-29-migrating_my_github-pages_blog_from_jekyl_to_hugo/</guid>
      <description>
        
          
            It’s been more than 5 years that I setup this blog on GitHub Pages. Technology has moved on and there are a few new interesting options available.
I ended up choosing Hugo because it works well with GitHub Pages, renders static websites at a great performance, offers lots of templates to choose from, and was relatively simple to setup. Its customization though was a bit tricker than I had hoped.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Using logistic regression to classify images</title>
      <link>https://mmlind.github.io/post/2017-12-26-using_logistic_regression_to_classify_images/</link>
      <pubDate>Tue, 26 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2017-12-26-using_logistic_regression_to_classify_images/</guid>
      <description>
        
          
            In this blog post I show how to use logistic regression to classify images. Logistic regression is a statistical method for binary classification, i.e. for analyzing the dependency of a binary outcome on one or more independent variables.
    In a previous blog post I described linear regression. If you’re not familiar with linear regression read that post first.
Let’s briefly recap:
Linear Regression Linear regression is a statistical method for predicting the value of a continuous dependent variable based on one or several explanatory variables.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Understanding linear regression</title>
      <link>https://mmlind.github.io/post/2017-03-05-understanding_linear_regression/</link>
      <pubDate>Sun, 05 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2017-03-05-understanding_linear_regression/</guid>
      <description>
        
          
            Regression represents one of the cornerstones of machine learning. Comprehending its logic and math provides a solid foundation for learning more advanced machine learning techniques such as neural networks.
    Linear regression is a statistical method for modeling the relationship between variables. It’s used for analyzing dependencies and predicting values.
The underlying idea of linear regression is simple: given a dataset, we want to find the dependency of one of the variables in the dataset on one, some or all of the other variables.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Deep Neural Network for MNIST Handwriting Recognition</title>
      <link>https://mmlind.github.io/post/2016-02-12-deep_neural_network_for_mnist_handwriting_recognition/</link>
      <pubDate>Fri, 12 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2016-02-12-deep_neural_network_for_mnist_handwriting_recognition/</guid>
      <description>
        
          
            I finally found some time to enhance my neural network to support deep learning. The network now masters a variable number of layers and is capable of running convolutional layers. The architecture is generic, light weight (very small memory footprint) and super fast. :-)
    In a previous blog post I wrote about a simple 3-Layer neural network for MNIST handwriting recognition that I built. Its architecture -- a 3-layer structure with exactly 1 hidden layer -- was fix.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Simple 3-Layer Neural Network for MNIST Handwriting Recognition</title>
      <link>https://mmlind.github.io/post/2015-08-09-simple_3-layer_neural_network_for_mnist_handwriting_recognition/</link>
      <pubDate>Sun, 09 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2015-08-09-simple_3-layer_neural_network_for_mnist_handwriting_recognition/</guid>
      <description>
        
          
            I&#39;ve extended my simple 1-Layer neural network to include a hidden layer and use the back propagation algorithm for updating connection weights. The size of the network (number of neurons per layer) is dynamic. It&#39;s accuracy in classifying the handwritten digits in the MNIST database improved from 85% to &amp;gt;91%.
    In a previous blog post I introduced a simple 1-Layer neural network for MNIST handwriting recognition. It was based on a single layer of perceptrons whose connection weights are adjusted during a supervised learning process.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Simple 1-Layer Neural Network for MNIST Handwriting Recognition</title>
      <link>https://mmlind.github.io/post/2015-07-15-simple_1-layer_neural_network_for_mnist_handwriting_recognition/</link>
      <pubDate>Wed, 15 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2015-07-15-simple_1-layer_neural_network_for_mnist_handwriting_recognition/</guid>
      <description>
        
          
            In this post I&#39;ll explore how to use a very simple 1-layer neural network to recognize the handwritten digits in the MNIST database.
    In my previous blog post I gave a brief introduction how neural networks basically work. In this post I want to apply this know-how and write some code to recognize handwritten digits in images.
Image Recognition For the computer an image is just a collection of pixels with different colors.
          
          
        
      </description>
    </item>
    
    <item>
      <title>What is a Neural Network?</title>
      <link>https://mmlind.github.io/post/2015-07-09-what_is_a_neural_network/</link>
      <pubDate>Thu, 09 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2015-07-09-what_is_a_neural_network/</guid>
      <description>
        
          
            When people talk about artificial intelligence and machine learning, they most often refer to (artificial) neural networks (ANN or NN). Let&#39;s explore some machine learning basics, without excessive math, purely from a programmer&#39;s perspective.
    A neural network is a computation model imaging the brain where individual nodes (neurons) form an organism (a network) to process information. In order to understand what this means and how this is special, let&#39;s start by looking at how a computer normally works.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Up and running!</title>
      <link>https://mmlind.github.io/post/2015-07-01-up_and_running/</link>
      <pubDate>Wed, 01 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mmlind.github.io/post/2015-07-01-up_and_running/</guid>
      <description>
        
          
            Finally! I launched my new blog about my hacking adventures in the world of artificial intelligence and machine learning.
Following a recommendation on Andrej Karpathy&#39;s blog I decided to use Jekyll and host it on Github.
Thanks to Barry Clark&#39;s post it took only a few minutes to setup.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
