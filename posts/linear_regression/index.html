
<!DOCTYPE html>
<html lang="en" data-figures="" class="page">
  <head>  <title>Understanding linear regression | Machine Intelligence</title>
  <meta charset='utf-8'>
  <meta name="generator" content="Hugo 0.75.1" />
  <meta name = 'viewport' content = 'width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no'>
  <meta http-equiv = 'X-UA-Compatible' content = 'IE=edge'>
<meta property = "og:locale" content = "en_US" />
<meta property="og:type" content="article">
<meta name="description" content="Understanding linear regression">
<meta name = "twitter:card" content = "summary" />
<meta name = "twitter:creator" content = "@">
<meta name = "twitter:title" content = "Understanding linear regression" />
<meta property = "og:url" content = "https://mmlind.github.io/posts/linear_regression/" />
<meta property = "og:title" content = "Understanding linear regression" />
<meta property = "og:description" content = "Understanding linear regression" />
<meta property = "og:image" content = "https://mmlind.github.io/images/linreg_blackboard.jpg" />
<link rel='apple-touch-icon' sizes='180x180' href='https://mmlind.github.io/icons/apple-touch-icon.png'>
<link rel='icon' type='image/png' sizes='32x32' href='https://mmlind.github.io/icons/favicon-32x32.png'>
<link rel='icon' type='image/png' sizes='16x16' href='https://mmlind.github.io/icons/favicon-16x16.png'>
<link rel='manifest' href='https://mmlind.github.io/icons/site.webmanifest'>
<link rel="mask-icon" href= 'https://mmlind.github.io/safari-pinned-tab.svg' color="#002538">
<meta name="msapplication-TileColor" content="#002538">
<meta name="theme-color" content="#002538">

  
  <script>
    MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
            fontCache: 'global'
        }
    };
</script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script> 
  
  <link rel='canonical' href='https://mmlind.github.io/posts/linear_regression/'>

    

    
    
    <link rel="preload" href="https://mmlind.github.io/css/styles.b50e6691e3da9247a70dc0a15fb4416401a59018cade4c3c786767c7b0390125bff594847ffd7b7e899c659055fe665dd16d9bc2dd268635de5789acca216ba2.css" integrity = "sha512-tQ5mkePakkenDcChX7RBZAGlkBjK3kw8eGdnx7A5ASW/9ZSEf/17fomcZZBV/mZd0W2bwt0mhjXeV4msyiFrog==" as="style" crossorigin="anonymous">
    <link rel="preload" href="https://mmlind.github.io/js/bundle.min.7333d63e045f01aaa61cad70ba4e3060064949b5a64c00b840c4e0c664bcdb02869222e7bde3800fe4911ccad719c06a19cb5f48c60048c0e382631fbd17cece.js" as="script" integrity=
    "sha512-czPWPgRfAaqmHK1wuk4wYAZJSbWmTAC4QMTgxmS82wKGkiLnveOAD&#43;SRHMrXGcBqGctfSMYASMDjgmMfvRfOzg==" crossorigin="anonymous">

    
    <link rel="stylesheet" type="text/css" href="https://mmlind.github.io/css/styles.b50e6691e3da9247a70dc0a15fb4416401a59018cade4c3c786767c7b0390125bff594847ffd7b7e899c659055fe665dd16d9bc2dd268635de5789acca216ba2.css" integrity="sha512-tQ5mkePakkenDcChX7RBZAGlkBjK3kw8eGdnx7A5ASW/9ZSEf/17fomcZZBV/mZd0W2bwt0mhjXeV4msyiFrog==" crossorigin="anonymous">
	  
  </head>
  
  
    
  
  <body data-code="7" data-lines="false" id="documentTop">

<header class = 'nav_header' >
  <nav class = 'nav'>
    <a href='https://mmlind.github.io/' class = 'nav_brand nav_item'>
      <img src="https://mmlind.github.io/logos/matt_blog_logo2020.png" class="logo">
      <div class = 'nav_close'>
        <div>
          <svg class="icon">
            <use xlink:href="#open-menu"></use>
          </svg>
          <svg class="icon">
            <use xlink:href="#closeme"></use>
          </svg>
        </div>
      </div>
    </a>
    <div class = 'nav_body nav_body_left'>
      
      
      <div class = 'nav_parent'>
        <a href = 'https://mmlind.github.io/' class = 'nav_item'>Blog </a>
      </div>
      <div class = 'nav_parent'>
        <a href = 'https://mmlind.github.io/about/' class = 'nav_item'>About </a>
      </div>
      
<div class='follow'>
  <a href='https://github.com/mmlind'>
    <svg class="icon">
      <use xlink:href="#github"></use>
    </svg>
  </a>
  
  <a href='https://www.linkedin.com/in/mmlind'>
    <svg class="icon">
      <use xlink:href="#linkedin"></use>
    </svg>
  </a>
<div class = 'color_mode'>
  <input type = 'checkbox' class = 'color_choice' id = 'mode'>
</div>

</div>

    </div>
  </nav>
</header>

    <main>
  
<div class = 'grid-inverse wrap content'>
  <article class='post_content'>
    <h1 class='post_title'>Understanding linear regression</h1><div class = 'post_meta'>
  <svg class="icon">
    <use xlink:href="#calendar"></use>
  </svg>
  <span class="post_date">
    Mar 5, 2017</span>
  <a href = 'https://mmlind.github.io/tags/machine-learning' class = 'post_tag button button_translucent'>Machine Learning
  </a>
  <a href = 'https://mmlind.github.io/tags/math' class = 'post_tag button button_translucent'>Math
  </a>
</div>

    <div class='post_share'>
  Share on:
  <a href="https://twitter.com/intent/tweet?text=Understanding%20linear%20regression&url=https%3a%2f%2fmmlind.github.io%2fposts%2flinear_regression%2f&tw_p=tweetbutton" class="twitter" title="Share on Twitter" target="_blank" rel="nofollow">
    <svg class="icon">
      <use xlink:href="#twitter"></use>
    </svg>
  </a>
  
  <a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fmmlind.github.io%2fposts%2flinear_regression%2f&t=Understanding%20linear%20regression" class="facebook" title="Share on Facebook" target="_blank" rel="nofollow">
    <svg class="icon">
      <use xlink:href="#facebook"></use>
    </svg>
  </a>
  <script>
    function shareViaLinkedin() {
      window.open('http://www.linkedin.com/shareArticle?mini=true&url='+encodeURIComponent("https://mmlind.github.io/posts/linear_regression/"), '', 'left=0,top=0,width=650,height=420,personalbar=0,toolbar=0,scrollbars=0,resizable=0');
    }
  </script>
  <a href="#linkedinshare" id = "linkedinshare" class="linkedin" title="Share on LinkedIn" rel="nofollow" onclick="shareViaLinkedin()">
    <svg class="icon">
      <use xlink:href="#linkedin"></use>
    </svg>
  </a>
  <a href="https://mmlind.github.io/posts/linear_regression/" title="Copy Link" class="link link_yank">
    <svg class="icon">
      <use xlink:href="#yank"></use>
    </svg>
  </a>
</div>

    
    <h1 id="heading"></h1>
<p>Regression represents one of the cornerstones of machine learning.
Comprehending its logic and math provides a solid foundation for learning more advanced machine learning techniques such as neural networks.</p>
<p><img src="/images/linreg_blackboard.jpg" alt=""></p>
<p>Linear regression is a statistical method for modeling the relationship between variables.
It&rsquo;s used for analyzing dependencies and predicting values.</p>
<p>The underlying idea of linear regression is simple: given a dataset, we want to find the dependency of one of the variables in the dataset on one, some or all of the other variables.</p>
<p>If we notate the dependent or target variable as $y$, and the independent or explanatory variables as $x$, what we&rsquo;re looking for is the function</p>
<p>$y = f(x)$</p>
<p>that, given an arbitrary value for $x$, outputs the desired target variable $y$.
The output represents our <em>prediction</em> for y. It&rsquo;s an estimation of the unknown actual value of $y$.</p>
<p>This function is also referred to as our <strong>model</strong>. We&rsquo;re modeling the relationship between $x$ and $y$ or, more specifically, the dependency of $y$ on $x$.</p>
<p>The dependency is learned from a <strong>dataset</strong>.
The dataset is simply a large collection of examples of <em>x</em> recorded in the real world.
These examples are used to <em>train</em> the computer to estimate the target function and hence called <strong>training examples</strong>.</p>
<p>Each training example may consist of just one or many explanatory variables $x$ plus their respective <em>&ldquo;to-be-predicted&rdquo;</em> target variable $y$.
It&rsquo;s typically a many-to-one relationship, i.e. many independent variables are mapped to a single dependent variable.</p>
<p>The objective of linear regression is to have the computer learn dependencies inside the data simply from looking at many examples.
Each example must include the <em>correct</em> or desired output for the respective inputs, also referred to as <strong>the ground truth</strong>.</p>
<p>This type of learning is therefore called <strong>supervised learning</strong>.
By providing correct examples of $y$ we&rsquo;re <em>supervising</em> the learning algorithm.</p>
<h2 id="the-methodology">The Methodology</h2>
<p>Linear regression, or regression analysis in general, consists of 3 steps: define a hypothesis function, define a cost function and run an optimization algorithm.</p>
<p>Before explaining these components in detail, let me briefly address a hurdle that many people face when learning regression or machine learning in general: the math.</p>
<p>If you don&rsquo;t have a strong math background a series of equations presented in a tutorial or paper may present an immediate turn-off.
I know. Why is that so?</p>
<p><strong>Math</strong> is a language, <strong>the language of nature</strong>. Unfortunately, the average reader is not fluent in or familiar with this language.
Mathematical equations therefore easily get intimidating because they may include unknown signs or greek letters.</p>
<p>Let&rsquo;s go step by step through the math, and you&rsquo;ll see it&rsquo;s not that hard at all.
Especially if you&rsquo;re a software developer and used to systematic thinking.</p>
<h3 id="mathematical-notation">Mathematical Notation</h3>
<p>A dataset consists of a number of training samples. Typically, the more the better.
Let&rsquo;s define or notate the size of the dataset as $m$:</p>
<p><code>m = number of training examples in the dataset</code></p>
<p>Every training sample consists of a number of independent or explanatory values, called <strong>features</strong>.
Let&rsquo;s notate the number of such features per training example as $n$:</p>
<p><code>n = number of features per training example</code></p>
<p>Then let&rsquo;s pack all of these features of a single training example into a vector.
In this context a vector can be regarded as simply a list or array of values.
Let&rsquo;s notate this vector as $x$:</p>
<p>$x = [x_1, x_2, &hellip; x_n]$</p>
<p>The subscript index denotes the j-th feature of a training example, where $j \in $ {1..n}.</p>
<p>But of course there are many training examples, not just a single one.
Therefore, we will use another index, a superscript $i$, to denote a specific training example $i$, where $i \in $ {1..m}.
A specific training example, with all its features, is then denoted as vector $x^{(i)}$:</p>
<p>$x^{(i)} = $ n-dimensional vector of all features of the i-th training sample</p>
<p>Combining both, the value of a specific feature of a specific training example is denoted as $x_{j}^{(i)}$:</p>
<p>$x_{j}^{(i)}$ = value of the $j^{th}$ feature in the $i^{th}$ training sample</p>
<p>So far so good. That wasn&rsquo;t so hard.</p>
<p>Now, what&rsquo;s missing? To compute, or estimate, the target variable linear regression <em>weighs</em> each individual feature differently and then adds up all the weighted features.</p>
<p>These weights are called the <strong>parameters</strong> and are notated with the greek letter &ldquo;Theta&rdquo; $\theta$.</p>
<p>Every feature gets weighted differently.
The number of parameters that we&rsquo;re trying to find is therefore the same as the number of features.</p>
<p>But we don&rsquo;t compute separate parameters for each training example.
What we want is a <em>single</em> model, one function that covers <em>all</em> of the training examples.
So let&rsquo;s pack the parameters into a vector as well and denote this vector as $\theta$:</p>
<p>$\theta = [\theta_1, \theta_2, &hellip; , \theta_n]$</p>
<p>Great. That&rsquo;s all we need. Now, let&rsquo;s get started with the 3 steps methodology mentioned above.</p>
<p><img src="/images/linreg_hypothesis.jpg" alt=""></p>
<h3 id="hypothesis-function">Hypothesis Function</h3>
<p>Regression attempts to model an unknown function $h$ that maps the given inputs $x$ to an output value $y$.
Let&rsquo;s call this our <strong>hypothesis function</strong> and denote it as</p>
<p>$h = f(x)$</p>
<p>Again, there could be a single input value x or many input values. (Remember, $x$ is a vector and this vector may have a size of 1 which would make $x$ a scalar.)</p>
<p>Depending on the number of explanatory variables $x$, we distinguish 3 types of linear regression:</p>
<ul>
<li>
<p>If there is only a <strong>single</strong> input variable or feature $x$, we call the model a <strong>uni-variate</strong> linear regression:
$$h_{\theta}(x) = \theta_{0} + \theta_{1} x $$</p>
</li>
<li>
<p>If there are <strong>multiple</strong> input variables or features $x$, we call the model a <strong>multi-variate</strong> linear regression:
$$h_{\theta}(x) = \theta_{0} + \theta_{1} x_{1} + \ &hellip; \theta_{i} x_{i} \ &hellip; + \ \theta_{n} x_{n}$$</p>
</li>
<li>
<p>If the independent variables or features include <strong>exponentials</strong>, we call the model a <strong>polynomial</strong> linear regression.
$$h_{\theta}(x) = \theta_{0} + \theta_{1} x_{1} + \theta_{2} x_{1}^2 + \theta_{3} x_{1}x_{2} + \theta_{4} x_{2}^2 \ &hellip; $$</p>
</li>
</ul>
<p>Math and methodology are the same for all three.</p>
<p>You may have noticed that I added an additional parameter $\theta_0$.
The reason is that the function that we&rsquo;re trying to find may have a certain <em>base</em> value that is completely independent from any of the features.
This base is also called the <strong>bias</strong>.</p>
<p>Since there is no feature mapped or related to the bias, we set the corresponding</p>
<p>$x_0 = 1$</p>
<p>Having added the bias parameter to our feature vector $x$ <em>and</em> to our parameter vector $\theta$, both vectors become of size <strong>n+1</strong>.</p>
<p><img src="/images/linreg_costfunction.jpg" alt=""></p>
<h3 id="cost-function">Cost Function</h3>
<p>After we defined the hypothesis function, we can feed data into this function and it will return a prediction or an estimate of what the output value y should be for a particular set of inputs. But how good is this value computed via our hypothesis?</p>
<p>We need to measure the <strong>accuracy</strong> of our hypothesis.
We do so by computing the difference of the hypothesis with the actual &ldquo;true&rdquo; target value from the dataset.
(Remember from above that in supervised learning we need to provide the target value for each training example.)</p>
<p>Since there are many training examples and not just a single one, we can measure the difference for each.
That&rsquo;s what a cost function does.</p>
<p>The difference of hypothesis and target is called the <strong>error</strong> or <strong>cost</strong> or <strong>loss</strong>.</p>
<p>Conceptually, there are many different options how to measure or express this error.
The most common method is to compute th <strong>mean squared error</strong> (MSE) and is defined as follows:</p>
<ul>
<li>compute the difference of hypothesis and correct output for a particular training example $i$,</li>
<li>square the difference,</li>
<li>sum the squared differences across all $m$ training examples, and then</li>
<li>divide the total by $m$ to get the mean (average) cost.</li>
</ul>
<p>Mathematically, we&rsquo;ll denote this kind of cost function $J$ as</p>
<p>$J(\theta_0, \theta_1, &hellip;, \theta_n) = \frac{1}{2m} \sum\limits_{i=1}^{m} ( h_{\theta} (x^{(i)}) - y^{(i)} )^2$</p>
<p>and read it as &ldquo;<em>J with respect to Theta 1, &hellip; Theta n</em>_&rdquo; stating in brackets the variable(s) that we&rsquo;re minimizing for (i.e. Theta).</p>
<p>Next, we want to <strong>minimize</strong> this cost function, i.e. find those parameters or those values of $\theta$ that result in the minimum J.</p>
<p>Minimizing the cost is an <strong>optimization problem</strong>: we want to find the optimal representation of $h(x)$ which leads us to the third and last step in the methodology for linear regression.</p>
<p>Before we do that, a short side note. You probably noticed that in above formula for J we&rsquo;re actually dividing by 2m instead of simply by m.
This is done for mathematical convenience later (when computing the derivate).
Since it doesn&rsquo;t impact the minimization it represents a valid simplification.</p>
<p>At this point it may also be worth noting that in addition to mean squared error there are other possible cost functions, e.g. one could compute the mean <em>absolute</em> error or the average cross entropy.</p>
<p><img src="/images/linreg_optimization.jpg" alt=""></p>
<h3 id="optimization-algorithm">Optimization Algorithm</h3>
<p>To find the optimal hypothesis we need to minimize the error or cost.
The optimal values for our parameters Theta will result in the lowest cost or lowest value for J.</p>
<p>There are different ways to do this.
The most popular optimization algorithm is called gradient descent.</p>
<h3 id="gradient-descent">Gradient Descent</h3>
<p>The idea of gradient descent is simple:</p>
<ul>
<li>Compute the slope at an arbitrary point of the cost curve.</li>
<li>If the slope is <em>bigger</em> than 0, slightly <em>decrease</em> the input values, i.e. pick a new point a little further to the <em>left</em> on the cost curve.</li>
<li>If the slope is <em>smaller</em> than 0, we slightly <em>increase</em> our input values, i.e. pick a new point a little further to the <em>right</em> on the cost curve.</li>
<li>Repeat these steps until the slope is so small that there is hardly any further reduction or improvement.</li>
</ul>
<p>Let&rsquo;s see how to do this mathematically.</p>
<p>The slope of a curve is computed via the derivative of the underlying function.
That&rsquo;s easy. But our function does not have one but <em>multiple</em> features and the same number of parameters.
Hence we need to compute the <strong>partial derivate</strong> for each of those parameters.</p>
<p>Gradient descent incrementally updates the <em>parameters</em> in our cost function using two components: the gradient and a step size.</p>
<p>The partial derivative (&ldquo;delta&rdquo;) of the cost function J</p>
<p>$J(\theta_0, \theta_1, &hellip;, \theta_n) = \frac{1}{2m} \sum\limits_{i=1}^{m} ( h_{\theta} (x^{(i)}) - y^{(i)} )^2$</p>
<p>with respect to parameter $\theta_j$ is called the <strong>gradient</strong>:</p>
<p>$\frac{\delta}{\delta \theta_j} J(\theta_0, \theta_1, &hellip;, \theta_n) = \frac{1}{m} \sum\limits_{i=1}^{m} ( h_{\theta} (x^{(i)}) - y^{(i)} ) \ x_j^{(i)} $</p>
<p>This gradient is used to update or change the parameters. It will be subtracted at an arbitrary &ldquo;step size&rdquo; called the <strong>learning rate</strong> alpha ($\alpha$).
So, we simultaneously update all parameters $\theta_j$ for j = {0..n}:</p>
<p>$\theta_j := \theta_j - \alpha \frac{\delta}{\delta \theta_j} J(\theta) = \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} ( h_{\theta} (x^{(i)}) - y^{(i)} ) \ x_j^{(i)} $</p>
<p>Gradient descent automatically takes smaller steps when it approaches the minimum because the slope gets flatter.</p>
<p>There are 3 types of gradient descent:</p>
<ul>
<li><strong>batch</strong> gradient descent = each step uses all training samples (=&gt; computationally expensive)</li>
<li><strong>stochastic</strong> gradient descent = each step uses only a single sample</li>
<li><strong>mini batch</strong> gradient descent = each step uses an arbitrary mini-batch size of training samples</li>
</ul>
<h4 id="batch-gradient-descent">Batch Gradient Descent</h4>
<p>Batch Gradient Descent considers <strong>all</strong> training examples for each optimization step. It directly converges to the global minimum.</p>
<h4 id="stochastic-gradient-descent">Stochastic Gradient Descent</h4>
<p>Stochastic gradient descent considers only a <strong>single</strong> training example at a time, therefore does not directly converge but zig-zags in on the global minimum. It does not actually reach the global minimum, but &ldquo;oscillates&rdquo; around it.</p>
<p>Looping through the whole dataset only once may not be sufficient to converge to a global optimum. In practice SGD is often run 1-10 times on a given dataset.</p>
<p>SGD is a type of <strong>online learning</strong> algorithm because it does not require a whole dataset at once but can process training examples one-by-one, i.e. learn <em>online</em>.</p>
<p><img src="/images/linreg_bigdata.jpg" alt=""></p>
<h4 id="mini-batch-gradient-descent">Mini Batch Gradient Descent</h4>
<p>Mini Batch Gradient Descent is a kind of hybrid version trying to combine the advantages of both batch and stochastic gradient descent. Instead of summing (or looping through) the whole dataset for each optimization step, the algorithm uses a <strong>randomly selected subset</strong> of examples.</p>
<p>Its main advantage is that it&rsquo;s computationally much less expensive. It also converges faster than stochastic gradient descent because it avoids (much of) the zig-zag.</p>
<p>Given today&rsquo;s parallel computing capabilities mini-batch is especially performant if the algorithm is vectorized.</p>
<p>Thanks for reading! Feel free to contact me for any questions or comments.</p>

    

  </article>
<aside class="sidebar">
  <section class="sidebar_inner">
    <h2>Matt Lind</h2>
    <div>
      CTO and passionate software engineer. Azure cloud and M365 global admin. Excited about parellel processing and distributed systems. AI enthusiast with special interest in natural language processing (NLP)...
    </div>
    <a href = 'https://mmlind.github.io/about/' class="button mt-1" role="button">Read More</a>
    <h2 class="mt-4">Featured Posts</h2>
    <ul>
    
    </ul>
    <h2 class="mt-4">Recent Posts</h2>
    <ul class="flex-column">
      
      <li>
        <a href="https://mmlind.github.io/posts/using_logistic_regression_to_solve_mnist/" class="nav-link">Using logistic regression to classify images</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/deep_neural_network_for_mnist_handwriting_recognition/" class="nav-link">Deep neural network for MNIST handwriting recognition</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/simple_3-layer_neural_network_for_mnist_handwriting_recognition/" class="nav-link">Simple 3-layer neural network for MNIST handwriting recognition</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/simple_1-layer_neural_network_for_mnist_handwriting_recognition/" class="nav-link">Simple 1-layer neural network for MNIST handwriting recognition</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/what_is_a_neural_network/" class="nav-link">What is a neural network?</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/up_and_running/" class="nav-link">Up and running!</a>
      </li>
    </ul> 
    
    
    
    
    
    
    <div>
      <h2 class="mt-4 taxonomy" id="tags-section">Tags</h2>
      <nav class="tags_nav">
        <a href='https://mmlind.github.io/tags/machine-learning/' class=" post_tag button button_translucent">
          MACHINE-LEARNING
          <span class='button_tally'>6</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/computer-vision/' class=" post_tag button button_translucent">
          COMPUTER-VISION
          <span class='button_tally'>5</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/math/' class=" post_tag button button_translucent">
          MATH
          <span class='button_tally'>2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/blogging/' class=" post_tag button button_translucent">
          BLOGGING
          <span class='button_tally'>1</span>
        </a>
        
        
      </nav>
    </div>
    
    
  </section>
</aside>

</div>
    </main><svg width="0" height="0" class="hidden">
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="facebook">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h151V331h-60v-90h60v-61c0-49.629 40.371-90 90-90h91v90h-91v61h91l-15 90h-76v181h121c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18.001 18.001" id="twitter">
    <path d="M15.891 4.013c.808-.496 1.343-1.173 1.605-2.034a8.68 8.68 0 0 1-2.351.861c-.703-.756-1.593-1.14-2.66-1.14-1.043 0-1.924.366-2.643 1.078a3.56 3.56 0 0 0-1.076 2.605c0 .309.039.585.117.819-3.076-.105-5.622-1.381-7.628-3.837-.34.601-.51 1.213-.51 1.846 0 1.301.549 2.332 1.645 3.089-.625-.053-1.176-.211-1.645-.47 0 .929.273 1.705.82 2.388a3.623 3.623 0 0 0 2.115 1.291c-.312.08-.641.118-.979.118-.312 0-.533-.026-.664-.083.23.757.664 1.371 1.291 1.841a3.652 3.652 0 0 0 2.152.743C4.148 14.173 2.625 14.69.902 14.69c-.422 0-.721-.006-.902-.038 1.697 1.102 3.586 1.649 5.676 1.649 2.139 0 4.029-.542 5.674-1.626 1.645-1.078 2.859-2.408 3.639-3.974a10.77 10.77 0 0 0 1.172-4.892v-.468a7.788 7.788 0 0 0 1.84-1.921 8.142 8.142 0 0 1-2.11.593z"
      ></path>
  </symbol>
  <symbol aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="mail">
    <path  d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="calendar">
    <path d="M452 40h-24V0h-40v40H124V0H84v40H60C26.916 40 0 66.916 0 100v352c0 33.084 26.916 60 60 60h392c33.084 0 60-26.916 60-60V100c0-33.084-26.916-60-60-60zm20 412c0 11.028-8.972 20-20 20H60c-11.028 0-20-8.972-20-20V188h432v264zm0-304H40v-48c0-11.028 8.972-20 20-20h24v40h40V80h264v40h40V80h24c11.028 0 20 8.972 20 20v48z"></path>
    <path d="M76 230h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 310h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 390h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80-80h40v40h-40z"></path>
  </symbol>
  <symbol viewBox="-21 0 512 512" xmlns="http://www.w3.org/2000/svg" id="yank">
    <path d="M410.668 405.332H165.332c-32.363 0-58.664-26.3-58.664-58.664v-288c0-32.363 26.3-58.668 58.664-58.668h181.504c21.059 0 41.687 8.535 56.555 23.445l42.496 42.496c15.125 15.125 23.445 35.223 23.445 56.575v224.152c0 32.363-26.3 58.664-58.664 58.664zM165.332 32c-14.7 0-26.664 11.969-26.664 26.668v288c0 14.7 11.965 26.664 26.664 26.664h245.336c14.7 0 26.664-11.965 26.664-26.664V122.516c0-12.82-4.992-24.871-14.059-33.942l-42.496-42.496C371.84 37.121 359.488 32 346.836 32zm0 0"></path>
    <path d="M314.668 512h-256C26.305 512 0 485.695 0 453.332V112c0-32.363 26.305-58.668 58.668-58.668h10.664c8.832 0 16 7.168 16 16s-7.168 16-16 16H58.668C43.968 85.332 32 97.301 32 112v341.332C32 468.032 43.969 480 58.668 480h256c14.7 0 26.664-11.969 26.664-26.668v-10.664c0-8.832 7.168-16 16-16s16 7.168 16 16v10.664c0 32.363-26.3 58.668-58.664 58.668zM368 181.332H208c-8.832 0-16-7.168-16-16s7.168-16 16-16h160c8.832 0 16 7.168 16 16s-7.168 16-16 16zm0 0"></path>
    <path d="M368 245.332H208c-8.832 0-16-7.168-16-16s7.168-16 16-16h160c8.832 0 16 7.168 16 16s-7.168 16-16 16zm0 64H208c-8.832 0-16-7.168-16-16s7.168-16 16-16h160c8.832 0 16 7.168 16 16s-7.168 16-16 16zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="github">
    <path d="M255.968 5.329C114.624 5.329 0 120.401 0 262.353c0 113.536 73.344 209.856 175.104 243.872 12.8 2.368 17.472-5.568 17.472-12.384 0-6.112-.224-22.272-.352-43.712-71.2 15.52-86.24-34.464-86.24-34.464-11.616-29.696-28.416-37.6-28.416-37.6-23.264-15.936 1.728-15.616 1.728-15.616 25.696 1.824 39.2 26.496 39.2 26.496 22.848 39.264 59.936 27.936 74.528 21.344 2.304-16.608 8.928-27.936 16.256-34.368-56.832-6.496-116.608-28.544-116.608-127.008 0-28.064 9.984-51.008 26.368-68.992-2.656-6.496-11.424-32.64 2.496-68 0 0 21.504-6.912 70.4 26.336 20.416-5.696 42.304-8.544 64.096-8.64 21.728.128 43.648 2.944 64.096 8.672 48.864-33.248 70.336-26.336 70.336-26.336 13.952 35.392 5.184 61.504 2.56 68 16.416 17.984 26.304 40.928 26.304 68.992 0 98.72-59.84 120.448-116.864 126.816 9.184 7.936 17.376 23.616 17.376 47.584 0 34.368-.32 62.08-.32 70.496 0 6.88 4.608 14.88 17.6 12.352C438.72 472.145 512 375.857 512 262.353 512 120.401 397.376 5.329 255.968 5.329z"></path>
  </symbol>
  <symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="rss">
    <circle cx="3.429" cy="20.571" r="3.429"></circle>
    <path d="M11.429 24h4.57C15.999 15.179 8.821 8.001 0 8v4.572c6.302.001 11.429 5.126 11.429 11.428z"></path>
    <path d="M24 24C24 10.766 13.234 0 0 0v4.571c10.714 0 19.43 8.714 19.43 19.429z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="linkedin">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h362c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zM181 406h-60V196h60zm0-240h-60v-60h60zm210 240h-60V286c0-16.54-13.46-30-30-30s-30 13.46-30 30v120h-60V196h60v11.309C286.719 202.422 296.93 196 316 196c40.691.043 75 36.547 75 79.688zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 612 612" id="arrow">
    <path d="M604.501 440.509L325.398 134.956c-5.331-5.357-12.423-7.627-19.386-7.27-6.989-.357-14.056 1.913-19.387 7.27L7.499 440.509c-9.999 10.024-9.999 26.298 0 36.323s26.223 10.024 36.222 0l262.293-287.164L568.28 476.832c9.999 10.024 26.222 10.024 36.221 0 9.999-10.023 9.999-26.298 0-36.323z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="carly">
    <path d="M504.971 239.029L448 182.059V84c0-46.317-37.682-84-84-84h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c19.851 0 36 16.149 36 36v108c0 6.365 2.529 12.47 7.029 16.971L454.059 256l-47.029 47.029A24.002 24.002 0 0 0 400 320v108c0 19.851-16.149 36-36 36h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c46.318 0 84-37.683 84-84v-98.059l56.971-56.971c9.372-9.372 9.372-24.568 0-33.941zM112 192V84c0-19.851 16.149-36 36-36h44c13.255 0 24-10.745 24-24S205.255 0 192 0h-44c-46.318 0-84 37.683-84 84v98.059l-56.971 56.97c-9.373 9.373-9.373 24.568 0 33.941L64 329.941V428c0 46.317 37.682 84 84 84h44c13.255 0 24-10.745 24-24s-10.745-24-24-24h-44c-19.851 0-36-16.149-36-36V320c0-6.365-2.529-12.47-7.029-16.971L57.941 256l47.029-47.029A24.002 24.002 0 0 0 112 192z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="copy">
    <path d="M366.345 406.069H66.207c-9.751 0-17.655-7.904-17.655-17.655V17.655C48.552 7.904 56.456 0 66.207 0h300.138C376.096 0 384 7.904 384 17.655v370.759c0 9.751-7.904 17.655-17.655 17.655z" fill="#f1f4fb"></path>
    <path d="M384 388.414v-76.992c-.907.322-1.869.494-2.751.882-6.307-6.593-14.733-11.025-24.111-11.964a40.49 40.49 0 0 0-8.448.039v-92.93c0-21.903-17.82-39.723-39.723-39.724a40.5 40.5 0 0 0-4.036.202c-20.012 2.004-35.689 19.916-35.689 40.781v101.773l-21.581 21.581c-15.241 15.241-21.393 36.866-16.456 57.847l3.802 16.16h131.338c9.75 0 17.655-7.905 17.655-17.655z"
      fill="#d5dced"></path>
    <circle cx="128" cy="105.931" r="26.483" fill="#b4e66e"></circle>
    <circle cx="128" cy="203.034" r="26.483" fill="#dae169"></circle>
    <circle cx="128" cy="300.138" r="26.483" fill="#ffdc64"></circle>
    <path d="M331.034 229.517H189.793c-4.879 0-8.828-3.953-8.828-8.828s3.948-8.828 8.828-8.828h141.241c4.879 0 8.828 3.953 8.828 8.828s-3.948 8.828-8.828 8.828z" fill="#7f8499"></path>
    <path d="M295.724 194.207H189.793a8.826 8.826 0 0 1-8.828-8.828 8.826 8.826 0 0 1 8.828-8.828h105.931a8.826 8.826 0 0 1 8.828 8.828 8.826 8.826 0 0 1-8.828 8.828z" fill="#5b5d6e"></path>
    <path d="M331.034 326.621H189.793c-4.879 0-8.828-3.953-8.828-8.828s3.948-8.828 8.828-8.828h141.241c4.879 0 8.828 3.953 8.828 8.828s-3.948 8.828-8.828 8.828z" fill="#7f8499"></path>
    <path d="M295.724 291.31H189.793c-4.879 0-8.828-3.953-8.828-8.828s3.948-8.828 8.828-8.828h105.931c4.879 0 8.828 3.953 8.828 8.828s-3.948 8.828-8.828 8.828z" fill="#5b5d6e"></path>
    <path d="M331.034 132.414H189.793a8.826 8.826 0 0 1-8.828-8.828 8.826 8.826 0 0 1 8.828-8.828h141.241a8.826 8.826 0 0 1 8.828 8.828 8.826 8.826 0 0 1-8.828 8.828z" fill="#7f8499"></path>
    <path d="M295.724 97.103H189.793a8.826 8.826 0 0 1-8.828-8.828 8.826 8.826 0 0 1 8.828-8.828h105.931a8.826 8.826 0 0 1 8.828 8.828 8.825 8.825 0 0 1-8.828 8.828z" fill="#5b5d6e"></path>
    <path d="M443.656 335.563c-13.21-1.323-24.345 9.015-24.345 21.954v-7.569c0-11.544-8.306-22.063-19.794-23.213-13.209-1.323-24.344 9.015-24.344 21.954v-7.569c0-11.544-8.306-22.063-19.794-23.213-13.209-1.323-24.344 9.015-24.344 21.954V207.448c0-12.939-11.135-23.277-24.345-21.954-11.486 1.15-19.793 11.669-19.793 23.213v109.086l-26.752 26.752a44.14 44.14 0 0 0-11.754 41.32l18.47 78.495c6.567 27.913 31.475 47.64 60.15 47.64h74.645c34.127 0 61.793-27.666 61.793-61.793v-91.431c-.001-11.544-8.306-22.063-19.793-23.213z"
      fill="#f0c087"></path>
    <path d="M339.862 361.377a8.829 8.829 0 0 0 8.828-8.828v-34.194c-10.052 2.061-17.655 10.844-17.655 21.506v12.687a8.827 8.827 0 0 0 8.827 8.829zM384 370.205a8.829 8.829 0 0 0 8.828-8.828v-34.194c-10.052 2.061-17.655 10.844-17.655 21.506v12.687a8.827 8.827 0 0 0 8.827 8.829zm44.138 8.827a8.829 8.829 0 0 0 8.828-8.828V336.01c-10.052 2.061-17.655 10.844-17.655 21.506v12.687a8.827 8.827 0 0 0 8.827 8.829zM288.885 464.36l-20.467-86.985a28.482 28.482 0 0 1 7.585-26.663l10.893-10.894v22.113a8.829 8.829 0 0 0 17.656 0V185.933c-10.344 2.173-17.655 11.972-17.655 22.773v109.087l-26.752 26.752a44.14 44.14 0 0 0-11.754 41.32l18.47 78.495c6.567 27.913 31.475 47.64 60.15 47.64h22.026c-28.677 0-53.584-19.727-60.152-47.64z"
      fill="#e6af78"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512.001 512.001" id="closeme">
    <path d="M284.286 256.002L506.143 34.144c7.811-7.811 7.811-20.475 0-28.285-7.811-7.81-20.475-7.811-28.285 0L256 227.717 34.143 5.859c-7.811-7.811-20.475-7.811-28.285 0-7.81 7.811-7.811 20.475 0 28.285l221.857 221.857L5.858 477.859c-7.811 7.811-7.811 20.475 0 28.285a19.938 19.938 0 0 0 14.143 5.857 19.94 19.94 0 0 0 14.143-5.857L256 284.287l221.857 221.857c3.905 3.905 9.024 5.857 14.143 5.857s10.237-1.952 14.143-5.857c7.811-7.811 7.811-20.475 0-28.285L284.286 256.002z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="open-menu">
    <path d="M492 236H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0-160H20C8.954 76 0 84.954 0 96s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0 320H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="instagram">
    <path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id=youtube>
    <path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z"/>
  </symbol>
</svg>
<footer class = 'footer'>
  <div class = 'footer_inner wrap pale'>
    <img src = 'https://mmlind.github.io/icons/apple-touch-icon.png' class = 'icon icon_2 transparent'>
    <p>Copyright &copy;&nbsp;<span class = 'year'>2020</span>&nbsp;MACHINE INTELLIGENCE. All Rights Reserved</p>
<a class='to_top' href="#documentTop">
  <svg class="icon">
    <use xlink:href="#arrow"></use>
  </svg>
</a>

  </div>
</footer>
    <script type="text/javascript" src = "https://mmlind.github.io/js/bundle.min.7333d63e045f01aaa61cad70ba4e3060064949b5a64c00b840c4e0c664bcdb02869222e7bde3800fe4911ccad719c06a19cb5f48c60048c0e382631fbd17cece.js" integrity=
    "sha512-czPWPgRfAaqmHK1wuk4wYAZJSbWmTAC4QMTgxmS82wKGkiLnveOAD&#43;SRHMrXGcBqGctfSMYASMDjgmMfvRfOzg==" crossorigin="anonymous"></script>
  </body>
</html>
