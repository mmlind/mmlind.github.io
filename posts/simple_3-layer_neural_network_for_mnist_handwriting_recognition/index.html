
<!DOCTYPE html>
<html lang="en" data-figures="" class="page">
  <head>  <title>Simple 3-layer neural network for MNIST handwriting recognition | Matt&#39;s Tech Blog</title>
  <meta charset='utf-8'>
  <meta name="generator" content="Hugo 0.75.1" />
  <meta name = 'viewport' content = 'width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no'>
  <meta http-equiv = 'X-UA-Compatible' content = 'IE=edge'>
<meta property = "og:locale" content = "en_US" />
<meta property="og:type" content="article">
<meta name="description" content="Simple 3-layer neural network for MNIST handwriting recognition">
<meta name = "twitter:card" content = "summary" />
<meta name = "twitter:creator" content = "@">
<meta name = "twitter:title" content = "Simple 3-layer neural network for MNIST handwriting recognition" />
<meta property = "og:url" content = "https://mmlind.github.io/posts/simple_3-layer_neural_network_for_mnist_handwriting_recognition/" />
<meta property = "og:title" content = "Simple 3-layer neural network for MNIST handwriting recognition" />
<meta property = "og:description" content = "Simple 3-layer neural network for MNIST handwriting recognition" />
<meta property = "og:image" content = "https://mmlind.github.io/images/mnist-3lnn-logo.jpg" />
<link rel='apple-touch-icon' sizes='180x180' href='https://mmlind.github.io/icons/apple-touch-icon.png'>
<link rel='icon' type='image/png' sizes='32x32' href='https://mmlind.github.io/icons/favicon-32x32.png'>
<link rel='icon' type='image/png' sizes='16x16' href='https://mmlind.github.io/icons/favicon-16x16.png'>
<link rel='manifest' href='https://mmlind.github.io/icons/site.webmanifest'>
<link rel="mask-icon" href= 'https://mmlind.github.io/safari-pinned-tab.svg' color="#002538">
<meta name="msapplication-TileColor" content="#002538">
<meta name="theme-color" content="#002538">

  
  <link rel='canonical' href='https://mmlind.github.io/posts/simple_3-layer_neural_network_for_mnist_handwriting_recognition/'>

    

    
    
    <link rel="preload" href="https://mmlind.github.io/css/styles.a5262c7798fc304069e7ad8c0884d94b91fa5f203c3219735fe37309b7080d17f29fc1a7be9842862d1a86cf9bca26cdf5eb570f6ad2d44bf92219908c2c44b4.css" integrity = "sha512-pSYsd5j8MEBp562MCITZS5H6XyA8MhlzX&#43;NzCbcIDRfyn8GnvphChi0ahs&#43;byibN9etXD2rS1Ev5IhmQjCxEtA==" as="style" crossorigin="anonymous">
    <link rel="preload" href="https://mmlind.github.io/js/bundle.min.7333d63e045f01aaa61cad70ba4e3060064949b5a64c00b840c4e0c664bcdb02869222e7bde3800fe4911ccad719c06a19cb5f48c60048c0e382631fbd17cece.js" as="script" integrity=
    "sha512-czPWPgRfAaqmHK1wuk4wYAZJSbWmTAC4QMTgxmS82wKGkiLnveOAD&#43;SRHMrXGcBqGctfSMYASMDjgmMfvRfOzg==" crossorigin="anonymous">

    
    <link rel="stylesheet" type="text/css" href="https://mmlind.github.io/css/styles.a5262c7798fc304069e7ad8c0884d94b91fa5f203c3219735fe37309b7080d17f29fc1a7be9842862d1a86cf9bca26cdf5eb570f6ad2d44bf92219908c2c44b4.css" integrity="sha512-pSYsd5j8MEBp562MCITZS5H6XyA8MhlzX&#43;NzCbcIDRfyn8GnvphChi0ahs&#43;byibN9etXD2rS1Ev5IhmQjCxEtA==" crossorigin="anonymous">
	  
  </head>
  
  
    
  
  <body data-code="7" data-lines="false" id="documentTop">

<header class = 'nav_header' >
  <nav class = 'nav'>
    <a href='https://mmlind.github.io/' class = 'nav_brand nav_item'>
      <img src="https://mmlind.github.io/logos/matt_blog_logo2020.png" class="logo">
      <div class = 'nav_close'>
        <div>
          <svg class="icon">
            <use xlink:href="#open-menu"></use>
          </svg>
          <svg class="icon">
            <use xlink:href="#closeme"></use>
          </svg>
        </div>
      </div>
    </a>
    <div class = 'nav_body nav_body_left'>
      
      
      <div class = 'nav_parent'>
        <a href = 'https://mmlind.github.io/' class = 'nav_item'>Blog </a>
      </div>
      <div class = 'nav_parent'>
        <a href = 'https://mmlind.github.io/about/' class = 'nav_item'>About </a>
      </div>
      
<div class='follow'>
  <a href='https://github.com/mmlind'>
    <svg class="icon">
      <use xlink:href="#github"></use>
    </svg>
  </a>
  
  <a href='https://www.linkedin.com/in/mmlind'>
    <svg class="icon">
      <use xlink:href="#linkedin"></use>
    </svg>
  </a>
<div class = 'color_mode'>
  <input type = 'checkbox' class = 'color_choice' id = 'mode'>
</div>

</div>

    </div>
  </nav>
</header>

    <main>
  
<div class = 'grid-inverse wrap content'>
  <article class='post_content'>
    <h1 class='post_title'>Simple 3-layer neural network for MNIST handwriting recognition</h1><div class = 'post_meta'>
  <svg class="icon">
    <use xlink:href="#calendar"></use>
  </svg>
  <span class="post_date">
    Aug 9, 2015</span>
  <a href = 'https://mmlind.github.io/tags/machine-learning' class = 'post_tag button button_translucent'>Machine Learning
  </a>
  <a href = 'https://mmlind.github.io/tags/computer-vision' class = 'post_tag button button_translucent'>Computer Vision
  </a>
</div>

    <div class='post_share'>
  Share on:
  <a href="https://twitter.com/intent/tweet?text=Simple%203-layer%20neural%20network%20for%20MNIST%20handwriting%20recognition&url=https%3a%2f%2fmmlind.github.io%2fposts%2fsimple_3-layer_neural_network_for_mnist_handwriting_recognition%2f&tw_p=tweetbutton" class="twitter" title="Share on Twitter" target="_blank" rel="nofollow">
    <svg class="icon">
      <use xlink:href="#twitter"></use>
    </svg>
  </a>
  
  <a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fmmlind.github.io%2fposts%2fsimple_3-layer_neural_network_for_mnist_handwriting_recognition%2f&t=Simple%203-layer%20neural%20network%20for%20MNIST%20handwriting%20recognition" class="facebook" title="Share on Facebook" target="_blank" rel="nofollow">
    <svg class="icon">
      <use xlink:href="#facebook"></use>
    </svg>
  </a>
  <script>
    function shareViaLinkedin() {
      window.open('http://www.linkedin.com/shareArticle?mini=true&url='+encodeURIComponent("https://mmlind.github.io/posts/simple_3-layer_neural_network_for_mnist_handwriting_recognition/"), '', 'left=0,top=0,width=650,height=420,personalbar=0,toolbar=0,scrollbars=0,resizable=0');
    }
  </script>
  <a href="#linkedinshare" id = "linkedinshare" class="linkedin" title="Share on LinkedIn" rel="nofollow" onclick="shareViaLinkedin()">
    <svg class="icon">
      <use xlink:href="#linkedin"></use>
    </svg>
  </a>
  <a href="https://mmlind.github.io/posts/simple_3-layer_neural_network_for_mnist_handwriting_recognition/" title="Copy Link" class="link link_yank">
    <svg class="icon">
      <use xlink:href="#yank"></use>
    </svg>
  </a>
</div>

    
    <h1 id="heading"></h1>
<p>I&rsquo;ve extended my simple 1-Layer neural network to include a hidden layer and use the back propagation algorithm for updating connection weights.
The size of the network (number of neurons per layer) is dynamic.
It&rsquo;s accuracy in classifying the handwritten digits in the MNIST database improved from 85% to &gt;91%.</p>
<p><img src="/images/mnist-3lnn-logo.jpg" alt=""></p>
<p>In a previous blog post I introduced a simple <a href="../simple_1-layer_neural_network_for_mnist_handwriting_recognition/">1-Layer neural network for MNIST handwriting recognition</a>.
It was based on a single layer of perceptrons whose connection weights are adjusted during a supervised learning process.
The result was an 85% accuracy in classifying the digits in the MNIST testing dataset.</p>
<p>The network did not use a <em>real</em> activation function (only simple linear &ldquo;normalization&rdquo;) and no weighted error back propagation.
Now, I want to add these things to see how this will improve the network&rsquo;s effectiveness.
At the same time I also want to make the code more versatile and re-usable, offering a standardized interface, and to allow for dyanmic network sizing.</p>
<p><img src="/images/3lnn.svg" alt=""></p>
<h2 id="from-1-layer-to-3-layers">From 1 Layer to 3 Layers</h2>
<p>The first major change in the design is adding a hidden layer between input and output.
So how come a 1 layer network becomes a 3 layer network?
It&rsquo;s simply a different naming convention.
The 1-layer network only had one single layer of perceptrons, the output layer.
In my previous design, the input to the network was NOT part of the network, i.e. whenever this input was needed (when calculating a node&rsquo;s output and when updating a node&rsquo;s weights) I refered to a <code>MNIST image</code>, a variable <em>outside</em> of the network.</p>
<p>When I redesigned the network I found it advantagous to include the input feed <em>inside</em> the network.
It allows to treat the network as an independent object (data structure) without external references.</p>
<p>Therefore, by adding a hidden layer, plus treating the input as a network layer as well, the 1-layer network becomes a 3-layer network.</p>
<h2 id="dynamically-sized-network">Dynamically Sized Network</h2>
<p>One of the challenges when redesigning the network was making its size dynamic.
In C, dynamically sized objects are not as easy to implement as in higher-level languages.</p>
<h3 id="the-data-model">The Data Model</h3>
<p>I decided to make use of a C feature originally refered to as the <em><a href="http://c-faq.com/struct/structhack.html">struct hack</a></em> until it officially became part of C as <em><a href="https://en.wikipedia.org/wiki/Flexible_array_member">flexible array member</a></em>.
The idea is simple: place an empty array <em>at the end</em> of a struct definition and manually allocate as much memory for the struct as it actually needs.</p>
<p>I make extensive use of this feature by stacking several layers of dynamically sized data structures.</p>
<p>Let&rsquo;s start at the smallest, most inner level of a network, the node. (<em>Node</em> refers to the same concept of a <em>perceptron</em> or <em>cell</em>, as I previously had called it.)</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="k">struct</span> <span class="n">Node</span><span class="p">{</span>
<span class="ln">2</span>    <span class="kt">double</span> <span class="n">bias</span><span class="p">;</span>
<span class="ln">3</span>    <span class="kt">double</span> <span class="n">output</span><span class="p">;</span>
<span class="ln">4</span>    <span class="kt">int</span> <span class="n">wcount</span><span class="p">;</span>
<span class="ln">5</span>    <span class="kt">double</span> <span class="n">weights</span><span class="p">[];</span>
<span class="ln">6</span><span class="p">};</span>
</code></pre></div><p>The number of weights of a node normally depends on the number of nodes in the previous layer (assuming a fully connected design).
Since we want this number to be dynamic we use an empty array or <em>flexible array member</em>.
But, in order to be able to access each individual weight later, we need to remember how many weights a node actually has.
This is what <code>wcount</code> (weight count) is for.</p>
<p>Next we put several of these nodes tegether to form a layer</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="k">struct</span> <span class="n">Layer</span><span class="p">{</span>
<span class="ln">2</span>    <span class="kt">int</span> <span class="n">ncount</span><span class="p">;</span>
<span class="ln">3</span>    <span class="n">Node</span> <span class="n">nodes</span><span class="p">[];</span>
<span class="ln">4</span><span class="p">};</span>
</code></pre></div><p>and use <code>ncount</code> (node count) to remember how many nodes are actually stored inside this layer.</p>
<p>Lastly, we stack several (for now only 3: input, hidden and output) of such layers into a <code>network</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="k">struct</span> <span class="n">Network</span><span class="p">{</span>
<span class="ln">2</span>    <span class="kt">int</span> <span class="n">inpNodeSize</span><span class="p">;</span>
<span class="ln">3</span>    <span class="kt">int</span> <span class="n">inpLayerSize</span><span class="p">;</span>
<span class="ln">4</span>    <span class="kt">int</span> <span class="n">hidNodeSize</span><span class="p">;</span>
<span class="ln">5</span>    <span class="kt">int</span> <span class="n">hidLayerSize</span><span class="p">;</span>
<span class="ln">6</span>    <span class="kt">int</span> <span class="n">outNodeSize</span><span class="p">;</span>
<span class="ln">7</span>    <span class="kt">int</span> <span class="n">outLayerSize</span><span class="p">;</span>
<span class="ln">8</span>    <span class="n">Layer</span> <span class="n">layers</span><span class="p">[];</span>
<span class="ln">9</span><span class="p">};</span>
</code></pre></div><p>Since the number of layers (for now) is fixed, i.e. always 3 (input, hidden, output), I did <em>not</em> add a variable <code>lcount</code> (layer count) to remember the number of layers. This could be done later if the number of (hidden) layers inside the network is required to be dynamic.</p>
<h3 id="manual-memory-allocation">Manual Memory Allocation</h3>
<p>Once the data model for the network and its components is defined, we need to manually allocate memory for each part.
If we assume that all layers are <em>fully connected</em>, i.e. each node connects to all nodes in the following layer, then the overall size of the network only depends on 3 numbers:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback"><span class="ln">1</span>1. Size of the input vector (= number of pixels of a MNIST image)
<span class="ln">2</span>2. Number of nodes in the hidden layer 
<span class="ln">3</span>3. Number of nodes in the output layer
</code></pre></div><p>Hence, we&rsquo;ll create an interface for creating the network which looks like this:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="n">Network</span> <span class="o">*</span><span class="nf">createNetwork</span><span class="p">(</span><span class="kt">int</span> <span class="n">size_of_input_vector</span><span class="p">,</span>    
<span class="ln">2</span>                       <span class="kt">int</span> <span class="n">number_of_nodes_in_hidden_layer</span><span class="p">,</span> 
<span class="ln">3</span>                       <span class="kt">int</span> <span class="n">number_of_nodes_in_output_layer</span><span class="p">);</span>
</code></pre></div><p>Now we calculate the size of each node type (<em>input</em>, <em>hidden</em>, <em>output</em>) as well as the required memory for each of the 3 layers. Adding up the layers' sizes then gives us the size of the overall network.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="n">Network</span> <span class="o">*</span><span class="nf">createNetwork</span><span class="p">(</span><span class="kt">int</span> <span class="n">inpCount</span><span class="p">,</span> <span class="kt">int</span> <span class="n">hidCount</span><span class="p">,</span> <span class="kt">int</span> <span class="n">outCount</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="c1">// Calculate size of INPUT Layer
</span><span class="ln"> 4</span><span class="c1"></span>    <span class="kt">int</span> <span class="n">inpNodeSize</span>     <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">);</span>         <span class="c1">// Input layer has 0 weights
</span><span class="ln"> 5</span><span class="c1"></span>    <span class="kt">int</span> <span class="n">inpLayerSize</span>    <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Layer</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">inpCount</span> <span class="o">*</span> <span class="n">inpNodeSize</span><span class="p">);</span>
<span class="ln"> 6</span>    
<span class="ln"> 7</span>    <span class="c1">// Calculate size of HIDDEN Layer
</span><span class="ln"> 8</span><span class="c1"></span>    <span class="kt">int</span> <span class="n">hidWeightsCount</span> <span class="o">=</span> <span class="n">inpCount</span><span class="p">;</span>
<span class="ln"> 9</span>    <span class="kt">int</span> <span class="n">hidNodeSize</span>     <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidWeightsCount</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
<span class="ln">10</span>    <span class="kt">int</span> <span class="n">hidLayerSize</span>    <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Layer</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">hidCount</span> <span class="o">*</span> <span class="n">hidNodeSize</span><span class="p">);</span>
<span class="ln">11</span>    
<span class="ln">12</span>    <span class="c1">// Calculate size of OUTPUT Layer
</span><span class="ln">13</span><span class="c1"></span>    <span class="kt">int</span> <span class="n">outWeightsCount</span> <span class="o">=</span> <span class="n">hidCount</span><span class="p">;</span>
<span class="ln">14</span>    <span class="kt">int</span> <span class="n">outNodeSize</span>     <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">outWeightsCount</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
<span class="ln">15</span>    <span class="kt">int</span> <span class="n">outLayerSize</span>    <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Layer</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">outCount</span> <span class="o">*</span> <span class="n">outNodeSize</span><span class="p">);</span>
<span class="ln">16</span>    
<span class="ln">17</span>    <span class="c1">// Allocate memory block for the network
</span><span class="ln">18</span><span class="c1"></span>    <span class="n">Network</span> <span class="o">*</span><span class="n">nn</span> <span class="o">=</span> <span class="p">(</span><span class="n">Network</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="n">Network</span><span class="p">)</span> <span class="o">+</span> <span class="n">inpLayerSize</span> <span class="o">+</span> <span class="n">hidLayerSize</span> <span class="o">+</span> <span class="n">outLayerSize</span><span class="p">);</span>
<span class="ln">19</span>    
<span class="ln">20</span>    <span class="k">return</span> <span class="n">nn</span><span class="p">;</span>
<span class="ln">21</span><span class="p">}</span>
<span class="ln">22</span>
</code></pre></div><p>Since we will need to refer to the aboves values throughout other parts of our code we store them as part of the network:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span>    <span class="n">nn</span><span class="o">-&gt;</span><span class="n">inpNodeSize</span>     <span class="o">=</span> <span class="n">inpNodeSize</span><span class="p">;</span>
<span class="ln">2</span>    <span class="n">nn</span><span class="o">-&gt;</span><span class="n">inpLayerSize</span>    <span class="o">=</span> <span class="n">inpLayerSize</span><span class="p">;</span>
<span class="ln">3</span>    <span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidNodeSize</span>     <span class="o">=</span> <span class="n">hidNodeSize</span><span class="p">;</span>
<span class="ln">4</span>    <span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidLayerSize</span>    <span class="o">=</span> <span class="n">hidLayerSize</span><span class="p">;</span>
<span class="ln">5</span>    <span class="n">nn</span><span class="o">-&gt;</span><span class="n">outNodeSize</span>     <span class="o">=</span> <span class="n">outNodeSize</span><span class="p">;</span>
<span class="ln">6</span>    <span class="n">nn</span><span class="o">-&gt;</span><span class="n">outLayerSize</span>    <span class="o">=</span> <span class="n">outLayerSize</span><span class="p">;</span>
<span class="ln">7</span>
</code></pre></div><p>Once the network is created, it&rsquo;s still only an empty memory block.
We now need to fill this memory block with the 3-tier layer structure.</p>
<p>The way we do this is by creating each layer as a temporary, independent object, i.e. in a different memory block.
Then we copy this memory block into the larger memory block allocated for the network and delete (<code>free</code>) the memory block of the temporary layer.</p>
<p>Obviously, the most critical point here is to copy the layer to the correct memory address as required by the data model.
Since we calculated each node&rsquo;s and each layer&rsquo;s size, this can be easily done using a <em>single byte pointer</em>.</p>
<p>I put this into a separate function and call it <code>initializing the network</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="kt">void</span> <span class="nf">initNetwork</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="kt">int</span> <span class="n">inpCount</span><span class="p">,</span> <span class="kt">int</span> <span class="n">hidCount</span><span class="p">,</span> <span class="kt">int</span> <span class="n">outCount</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="c1">// Copy the input layer into the network&#39;s memory block and delete it
</span><span class="ln"> 4</span><span class="c1"></span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">il</span> <span class="o">=</span> <span class="n">createInputLayer</span><span class="p">(</span><span class="n">inpCount</span><span class="p">);</span>
<span class="ln"> 5</span>    <span class="n">memcpy</span><span class="p">(</span><span class="n">nn</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">,</span><span class="n">il</span><span class="p">,</span><span class="n">nn</span><span class="o">-&gt;</span><span class="n">inpLayerSize</span><span class="p">);</span>
<span class="ln"> 6</span>    <span class="n">free</span><span class="p">(</span><span class="n">il</span><span class="p">);</span>
<span class="ln"> 7</span>    
<span class="ln"> 8</span>    <span class="c1">// Move pointer to end of input layer = beginning of hidden layer
</span><span class="ln"> 9</span><span class="c1"></span>    <span class="n">uint8_t</span> <span class="o">*</span><span class="n">sbptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">;</span>     <span class="c1">// single byte pointer
</span><span class="ln">10</span><span class="c1"></span>    <span class="n">sbptr</span> <span class="o">+=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">inpLayerSize</span><span class="p">;</span>
<span class="ln">11</span>    
<span class="ln">12</span>    <span class="c1">// Copy the hidden layer into the network&#39;s memory block and delete it
</span><span class="ln">13</span><span class="c1"></span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">hl</span> <span class="o">=</span> <span class="n">createLayer</span><span class="p">(</span><span class="n">hidCount</span><span class="p">,</span> <span class="n">inpCount</span><span class="p">);</span>
<span class="ln">14</span>    <span class="n">memcpy</span><span class="p">(</span><span class="n">sbptr</span><span class="p">,</span><span class="n">hl</span><span class="p">,</span><span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidLayerSize</span><span class="p">);</span>
<span class="ln">15</span>    <span class="n">free</span><span class="p">(</span><span class="n">hl</span><span class="p">);</span>
<span class="ln">16</span>    
<span class="ln">17</span>    <span class="c1">// Move pointer to end of hidden layer = beginning of output layer
</span><span class="ln">18</span><span class="c1"></span>    <span class="n">sbptr</span> <span class="o">+=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidLayerSize</span><span class="p">;</span>
<span class="ln">19</span>    
<span class="ln">20</span>    <span class="c1">// Copy the output layer into the network&#39;s memory block and delete it
</span><span class="ln">21</span><span class="c1"></span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">ol</span> <span class="o">=</span> <span class="n">createLayer</span><span class="p">(</span><span class="n">outCount</span><span class="p">,</span> <span class="n">hidCount</span><span class="p">);</span>
<span class="ln">22</span>    <span class="n">memcpy</span><span class="p">(</span><span class="n">sbptr</span><span class="p">,</span><span class="n">ol</span><span class="p">,</span><span class="n">nn</span><span class="o">-&gt;</span><span class="n">outLayerSize</span><span class="p">);</span>
<span class="ln">23</span>    <span class="n">free</span><span class="p">(</span><span class="n">ol</span><span class="p">);</span>
<span class="ln">24</span>        
<span class="ln">25</span><span class="p">}</span>
</code></pre></div><p>Let&rsquo;s look at this in more detail:
A <em>single byte pointer</em> is simply a pointer pointing to memory blocks of byte size 1.
I.e. we can easily move the pointer throughout the address space either by incrementing it via <code>pointer++</code> or by adding the number of bytes that we want the pointer to move forward by <code>pointer += bytes_to_move_forward</code>.</p>
<p>First, we define the pointer and make it point to the memory address where the first layer, the <em>input</em> layer, should be located:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="n">uint8_t</span> <span class="o">*</span><span class="n">sbptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">;</span>     <span class="c1">// single byte pointer
</span></code></pre></div><p>Then, we move the pointer forward to the memory address of the 2nd layer, the <em>hidden</em> layer, simply by increasing the pointer by the size of the input layer:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="n">sbptr</span> <span class="o">+=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">inpLayerSize</span><span class="p">;</span>
</code></pre></div><p>Above should help to understand the above <code>initNetwork()</code> function.
Inside of it we are creating the layers by calling a <code>createInputLayer()</code> and <code>createLayer()</code> function.
These functions will create and return (a pointer to) a <code>Layer</code> object which is already pre-filled with default values.</p>
<p>Let&rsquo;s start with the <code>createInputLayer()</code> function:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="n">Layer</span> <span class="o">*</span><span class="nf">createInputLayer</span><span class="p">(</span><span class="kt">int</span> <span class="n">inpCount</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="kt">int</span> <span class="n">inpNodeSize</span>     <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">);</span>         <span class="c1">// Input layer has 0 weights
</span><span class="ln"> 4</span><span class="c1"></span>    <span class="kt">int</span> <span class="n">inpLayerSize</span>    <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Layer</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">inpCount</span> <span class="o">*</span> <span class="n">inpNodeSize</span><span class="p">);</span>
<span class="ln"> 5</span>    
<span class="ln"> 6</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">il</span> <span class="o">=</span> <span class="n">malloc</span><span class="p">(</span><span class="n">inpLayerSize</span><span class="p">);</span>
<span class="ln"> 7</span>    <span class="n">il</span><span class="o">-&gt;</span><span class="n">ncount</span> <span class="o">=</span> <span class="n">inpCount</span><span class="p">;</span>
<span class="ln"> 8</span>    
<span class="ln"> 9</span>    <span class="c1">// Create a detault input layer node
</span><span class="ln">10</span><span class="c1"></span>    <span class="n">Node</span> <span class="n">iln</span><span class="p">;</span>
<span class="ln">11</span>    <span class="n">iln</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln">12</span>    <span class="n">iln</span><span class="p">.</span><span class="n">output</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln">13</span>    <span class="n">iln</span><span class="p">.</span><span class="n">wcount</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln">14</span>    
<span class="ln">15</span>    <span class="c1">// Use a single byte pointer to fill in the network&#39;s content
</span><span class="ln">16</span><span class="c1"></span>    <span class="n">uint8_t</span> <span class="o">*</span><span class="n">sbptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span> <span class="n">il</span><span class="o">-&gt;</span><span class="n">nodes</span><span class="p">;</span>
<span class="ln">17</span>    
<span class="ln">18</span>    <span class="c1">// Copy the default input layer node x times
</span><span class="ln">19</span><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">il</span><span class="o">-&gt;</span><span class="n">ncount</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">){</span>
<span class="ln">20</span>        <span class="n">memcpy</span><span class="p">(</span><span class="n">sbptr</span><span class="p">,</span><span class="o">&amp;</span><span class="n">iln</span><span class="p">,</span><span class="n">inpNodeSize</span><span class="p">);</span>
<span class="ln">21</span>        <span class="n">sbptr</span> <span class="o">+=</span> <span class="n">inpNodeSize</span><span class="p">;</span>
<span class="ln">22</span>    <span class="p">}</span>
<span class="ln">23</span>    
<span class="ln">24</span>    <span class="k">return</span> <span class="n">il</span><span class="p">;</span>
<span class="ln">25</span><span class="p">}</span>
</code></pre></div><p>The <em>input</em> layer is different from the other 2 layers (<em>hidden</em> and <em>output</em>) in two aspects:</p>
<ol>
<li>
<p>As I outlined above, the input layer is strictly speaking not a real network layer.
I.e. its nodes are not perceptrons because this layer does not have any connections, and hence any weights, to a previous layer.
For our coding this means that the size of a node&rsquo;s weight array (<code>weights[]</code>) is 0 and therefore the size of an input node is the same as the <em>actual</em> size of a node struct <code>sizeof(Node)</code>.</p>
</li>
<li>
<p>The second major difference of the input layer is that its main objective is storing the input that is fed into the network.
But for coding consistency and simplicity I want to use the same data model with the same naming conventions for the input layer as for the other two layers.
Therefore, I use the node&rsquo;s <code>output</code> variable to store the network&rsquo;s <em>input</em>.
(Keep this point in mind for later!)</p>
</li>
</ol>
<p>Next, let&rsquo;s look at the function how to create a <em>hidden</em> and an <em>output</em> layer.
Since both layers share the same structure we can use the same function.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="n">Layer</span> <span class="o">*</span><span class="nf">createLayer</span><span class="p">(</span><span class="kt">int</span> <span class="n">nodeCount</span><span class="p">,</span> <span class="kt">int</span> <span class="n">weightCount</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="kt">int</span> <span class="n">nodeSize</span> <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">weightCount</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
<span class="ln"> 4</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">l</span> <span class="o">=</span> <span class="p">(</span><span class="n">Layer</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="n">Layer</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">nodeCount</span><span class="o">*</span><span class="n">nodeSize</span><span class="p">));</span>
<span class="ln"> 5</span>    
<span class="ln"> 6</span>    <span class="n">l</span><span class="o">-&gt;</span><span class="n">ncount</span> <span class="o">=</span> <span class="n">nodeCount</span><span class="p">;</span>
<span class="ln"> 7</span>    
<span class="ln"> 8</span>    <span class="c1">// create a detault node
</span><span class="ln"> 9</span><span class="c1"></span>    <span class="n">Node</span> <span class="o">*</span><span class="n">dn</span> <span class="o">=</span> <span class="p">(</span><span class="n">Node</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">)</span> <span class="o">+</span> <span class="p">((</span><span class="n">weightCount</span><span class="p">)</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">)));</span>
<span class="ln">10</span>    <span class="n">dn</span><span class="o">-&gt;</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln">11</span>    <span class="n">dn</span><span class="o">-&gt;</span><span class="n">output</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln">12</span>    <span class="n">dn</span><span class="o">-&gt;</span><span class="n">wcount</span> <span class="o">=</span> <span class="n">weightCount</span><span class="p">;</span>
<span class="ln">13</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">o</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">o</span><span class="o">&lt;</span><span class="n">weightCount</span><span class="p">;</span><span class="n">o</span><span class="o">++</span><span class="p">)</span> <span class="n">dn</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">[</span><span class="n">o</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">// will be initialized later
</span><span class="ln">14</span><span class="c1"></span>    
<span class="ln">15</span>    <span class="n">uint8_t</span> <span class="o">*</span><span class="n">sbptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span> <span class="n">l</span><span class="o">-&gt;</span><span class="n">nodes</span><span class="p">;</span>     <span class="c1">// single byte pointer
</span><span class="ln">16</span><span class="c1"></span>    
<span class="ln">17</span>    <span class="c1">// copy the default node into the layer
</span><span class="ln">18</span><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">nodeCount</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="n">memcpy</span><span class="p">(</span><span class="n">sbptr</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="n">nodeSize</span><span class="p">),</span><span class="n">dn</span><span class="p">,</span><span class="n">nodeSize</span><span class="p">);</span>
<span class="ln">19</span>    
<span class="ln">20</span>    <span class="n">free</span><span class="p">(</span><span class="n">dn</span><span class="p">);</span>
<span class="ln">21</span>    
<span class="ln">22</span>    <span class="k">return</span> <span class="n">l</span><span class="p">;</span>
<span class="ln">23</span><span class="p">}</span>
</code></pre></div><p>The above code first creates an empty memory block for the layer and then fills it with n copies of an empty <em>default node</em>.</p>
<p>Lastly, the nodes' weights must be initialized with random values which we do via the following function:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback"><span class="ln"> 1</span>void initWeights(Network *nn, LayerType ltype){
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    int nodeSize = 0;
<span class="ln"> 4</span>    if (ltype==HIDDEN) nodeSize=nn-&gt;hidNodeSize;
<span class="ln"> 5</span>                  else nodeSize=nn-&gt;outNodeSize;
<span class="ln"> 6</span>    
<span class="ln"> 7</span>    Layer *l = getLayer(nn, ltype);
<span class="ln"> 8</span>    
<span class="ln"> 9</span>    uint8_t *sbptr = (uint8_t*) l-&gt;nodes;
<span class="ln">10</span>    
<span class="ln">11</span>    for (int o=0; o&lt;l-&gt;ncount;o++){
<span class="ln">12</span>    
<span class="ln">13</span>        Node *n = (Node *)sbptr;
<span class="ln">14</span>        
<span class="ln">15</span>        for (int i=0; i&lt;n-&gt;wcount; i++){
<span class="ln">16</span>            n-&gt;weights[i] = rand()/(double)(RAND_MAX);
<span class="ln">17</span>        }
<span class="ln">18</span>        
<span class="ln">19</span>        // init bias weight
<span class="ln">20</span>        n-&gt;bias =  rand()/(double)(RAND_MAX);
<span class="ln">21</span>        
<span class="ln">22</span>        sbptr += nodeSize;
<span class="ln">23</span>    }
<span class="ln">24</span>    
<span class="ln">25</span>}
</code></pre></div><p>I&rsquo;ll come back to this point of random initialization later (see below) as I had to find out how these initial values greatly impact network performance.</p>
<h2 id="training-the-network">Training the Network</h2>
<p>Once the network has been created and pre-filled with random weights we can start training it.
The training algorithm uses the following steps:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback"><span class="ln">1</span>
<span class="ln">2</span>1. Feed image data into the network
<span class="ln">3</span>2. Calculate node outputs of *hidden* and *output* layers (=FEED FORWARD)
<span class="ln">4</span>3. Back-propagate the error and adjust the weights (=FEED BACKWARD)
<span class="ln">5</span>4. Classify the image (*guess* what digit is presented in the image)
<span class="ln">6</span>
</code></pre></div><h3 id="1-feeding-data-into-the-network">1. Feeding Data into the Network</h3>
<p>As outlined in my <a href="../simple_1-layer_neural_network_for_mnist_handwriting_recognition/">previous MNIST code example</a> the data we want to feed into the network exists in the form of a <code>MNIST_Image</code> structure which holds a 28*28 pixel image.</p>
<p>In order to maintain a standardized interface for this neural network code, I don&rsquo;t want to feed the <code>MNIST_Image</code> object directly into the network.
Instead, I first convert it to a neutral <code>Vector</code> structure.</p>
<p>The <code>Vector</code> makes again use of C&rsquo;s <em>flexible array member</em> functionality so that it can be of dynamic size.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="k">struct</span> <span class="n">Vector</span><span class="p">{</span>
<span class="ln">2</span>    <span class="kt">int</span> <span class="n">size</span><span class="p">;</span>
<span class="ln">3</span>    <span class="kt">double</span> <span class="n">vals</span><span class="p">[];</span>
<span class="ln">4</span><span class="p">};</span>
</code></pre></div><p>We convert the <code>MNIST_Image</code> structure into this <code>Vector</code> structure using the following function:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="n">Vector</span> <span class="o">*</span><span class="nf">getVectorFromImage</span><span class="p">(</span><span class="n">MNIST_Image</span> <span class="o">*</span><span class="n">img</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="n">Vector</span> <span class="o">*</span><span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">Vector</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="n">Vector</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">)));</span>
<span class="ln"> 4</span>    
<span class="ln"> 5</span>    <span class="n">v</span><span class="o">-&gt;</span><span class="n">size</span> <span class="o">=</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">;</span>
<span class="ln"> 6</span>    
<span class="ln"> 7</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">v</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="ln"> 8</span>        <span class="n">v</span><span class="o">-&gt;</span><span class="n">vals</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">img</span><span class="o">-&gt;</span><span class="n">pixel</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">?</span> <span class="mi">1</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln"> 9</span>    
<span class="ln">10</span>    <span class="k">return</span> <span class="n">v</span><span class="p">;</span>
<span class="ln">11</span><span class="p">}</span>
</code></pre></div><p>Now we can feed this input vector into the network:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="kt">void</span> <span class="nf">feedInput</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="n">Vector</span> <span class="o">*</span><span class="n">v</span><span class="p">)</span> <span class="p">{</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">il</span><span class="p">;</span>
<span class="ln"> 4</span>    <span class="n">il</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">;</span>
<span class="ln"> 5</span>    
<span class="ln"> 6</span>    <span class="n">Node</span> <span class="o">*</span><span class="n">iln</span><span class="p">;</span>
<span class="ln"> 7</span>    <span class="n">iln</span> <span class="o">=</span> <span class="n">il</span><span class="o">-&gt;</span><span class="n">nodes</span><span class="p">;</span>
<span class="ln"> 8</span>    
<span class="ln"> 9</span>    <span class="c1">// Copy the vector content to the &#34;output&#34; field of the input layer nodes
</span><span class="ln">10</span><span class="c1"></span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">v</span><span class="o">-&gt;</span><span class="n">size</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">){</span>
<span class="ln">11</span>        <span class="n">iln</span><span class="o">-&gt;</span><span class="n">output</span> <span class="o">=</span> <span class="n">v</span><span class="o">-&gt;</span><span class="n">vals</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="ln">12</span>        <span class="n">iln</span><span class="o">++</span><span class="p">;</span>           
<span class="ln">13</span>    <span class="p">}</span>
<span class="ln">14</span>    
<span class="ln">15</span><span class="p">}</span>
</code></pre></div><p>As I explained above, I chose to utilize the <code>output</code> field of an input node to hold the image&rsquo;s pixels.</p>
<h3 id="2-feed-forward">2. Feed Forward</h3>
<p>Once the network is filled with an image&rsquo;s pixels, we can start <em>feeding</em> this data <em>forward</em>, from the input layer to the hidden layer and from the hidden layer to the output layer.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="kt">void</span> <span class="nf">feedForwardNetwork</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">){</span>
<span class="ln">2</span>    <span class="n">calcLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">HIDDEN</span><span class="p">);</span>
<span class="ln">3</span>    <span class="n">calcLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">OUTPUT</span><span class="p">);</span>
<span class="ln">4</span><span class="p">}</span>
</code></pre></div><p><em>Feed forward</em> means calculating the output values of each node by multiplying its weights with the output values of the previous layer&rsquo;s nodes that it is connected to.
This mechanism is the same as in my simple <a href="../simple_1-layer_neural_network_for_mnist_handwriting_recognition//">1-Layer NN MNIST code</a> so feel free to check there if below is not clear.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="kt">void</span> <span class="nf">calcLayer</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="n">LayerType</span> <span class="n">ltype</span><span class="p">){</span>
<span class="ln">2</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">l</span><span class="p">;</span>
<span class="ln">3</span>    <span class="n">l</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">ltype</span><span class="p">);</span>
<span class="ln">4</span>    
<span class="ln">5</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">l</span><span class="o">-&gt;</span><span class="n">ncount</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">){</span>
<span class="ln">6</span>        <span class="n">calcNodeOutput</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">ltype</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
<span class="ln">7</span>        <span class="n">activateNode</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span><span class="n">ltype</span><span class="p">,</span><span class="n">i</span><span class="p">);</span>
<span class="ln">8</span>    <span class="p">}</span>
<span class="ln">9</span><span class="p">}</span>
</code></pre></div><p>Calculating the <em>hidden</em> layer and the <em>output</em> layer works the same way, hence we can use the same function.
However, in order to be able to know which <em>type of layer</em> we&rsquo;re working on we&rsquo;re introducing a</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="k">typedef</span> <span class="k">enum</span> <span class="n">LayerType</span> <span class="p">{</span><span class="n">INPUT</span><span class="p">,</span> <span class="n">HIDDEN</span><span class="p">,</span> <span class="n">OUTPUT</span><span class="p">}</span> <span class="n">LayerType</span><span class="p">;</span>
</code></pre></div><p>and pass it as an argument to the <code>calcLayer()</code> function which itself applies the following steps to each of its nodes:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback"><span class="ln">1</span>1. Calculating the node&#39;s output
<span class="ln">2</span>2. Executing an &#39;activation function&#39; on the node&#39;s output
</code></pre></div><p>So, let&rsquo;s first calculate a node&rsquo;s output value:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="kt">void</span> <span class="nf">calcNodeOutput</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="n">LayerType</span> <span class="n">ltype</span><span class="p">,</span> <span class="kt">int</span> <span class="n">id</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">calcLayer</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">ltype</span><span class="p">);</span>
<span class="ln"> 4</span>    <span class="n">Node</span> <span class="o">*</span><span class="n">calcNode</span> <span class="o">=</span> <span class="n">getNode</span><span class="p">(</span><span class="n">calcLayer</span><span class="p">,</span> <span class="n">id</span><span class="p">);</span>
<span class="ln"> 5</span>    
<span class="ln"> 6</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">prevLayer</span><span class="p">;</span>
<span class="ln"> 7</span>    <span class="kt">int</span> <span class="n">prevLayerNodeSize</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln"> 8</span>    
<span class="ln"> 9</span>    <span class="k">if</span> <span class="p">(</span><span class="n">ltype</span><span class="o">==</span><span class="n">HIDDEN</span><span class="p">)</span> <span class="p">{</span>
<span class="ln">10</span>        <span class="n">prevLayer</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">INPUT</span><span class="p">);</span>
<span class="ln">11</span>        <span class="n">prevLayerNodeSize</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">inpNodeSize</span><span class="p">;</span>
<span class="ln">12</span>    <span class="p">}</span>
<span class="ln">13</span>    <span class="k">else</span> <span class="p">{</span>
<span class="ln">14</span>        <span class="n">prevLayer</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">HIDDEN</span><span class="p">);</span>
<span class="ln">15</span>        <span class="n">prevLayerNodeSize</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidNodeSize</span><span class="p">;</span>
<span class="ln">16</span>    <span class="p">}</span>
<span class="ln">17</span>    
<span class="ln">18</span>    <span class="n">uint8_t</span> <span class="o">*</span><span class="n">sbptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span> <span class="n">prevLayer</span><span class="o">-&gt;</span><span class="n">nodes</span><span class="p">;</span>
<span class="ln">19</span>    
<span class="ln">20</span>    <span class="c1">// Start by adding the bias
</span><span class="ln">21</span><span class="c1"></span>    <span class="n">calcNode</span><span class="o">-&gt;</span><span class="n">output</span> <span class="o">=</span> <span class="n">calcNode</span><span class="o">-&gt;</span><span class="n">bias</span><span class="p">;</span>
<span class="ln">22</span>    
<span class="ln">23</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">prevLayer</span><span class="o">-&gt;</span><span class="n">ncount</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">){</span>
<span class="ln">24</span>        <span class="n">Node</span> <span class="o">*</span><span class="n">prevLayerNode</span> <span class="o">=</span> <span class="p">(</span><span class="n">Node</span><span class="o">*</span><span class="p">)</span><span class="n">sbptr</span><span class="p">;</span>
<span class="ln">25</span>        <span class="n">calcNode</span><span class="o">-&gt;</span><span class="n">output</span> <span class="o">+=</span> <span class="n">prevLayerNode</span><span class="o">-&gt;</span><span class="n">output</span> <span class="o">*</span> <span class="n">calcNode</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="ln">26</span>        <span class="n">sbptr</span> <span class="o">+=</span> <span class="n">prevLayerNodeSize</span><span class="p">;</span>
<span class="ln">27</span>    <span class="p">}</span>
<span class="ln">28</span>
<span class="ln">29</span><span class="p">}</span>
</code></pre></div><p>To do this we need to loop through the node&rsquo;s connections, i.e. loop through its weights and the output values in the previous layer that it is connected to.
However, since we defined the network&rsquo;s components (layer, nodes, weights) using <em>flexible array members</em> the compiler does not know where one component ends and where the next one starts.
Hence, it is <strong>not</strong> possible to locate, for example, a node by simply referring to the n&rsquo;th member of the node array: <del>layer.node[n]</del>!</p>
<p>Since both a layer&rsquo;s and a node&rsquo;s size are unknown to the compiler I use a <em>single byte pointer</em> to navigate through the allocated memory space.
The above code does so by calling a <code>getLayer()</code> and <code>getNode()</code> function to access a particular layer or a particular node inside the network.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="n">Node</span> <span class="o">*</span><span class="nf">getNode</span><span class="p">(</span><span class="n">Layer</span> <span class="o">*</span><span class="n">l</span><span class="p">,</span> <span class="kt">int</span> <span class="n">nodeId</span><span class="p">)</span> <span class="p">{</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="kt">int</span> <span class="n">nodeSize</span> <span class="o">=</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">l</span><span class="o">-&gt;</span><span class="n">nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">wcount</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
<span class="ln"> 4</span>    <span class="n">uint8_t</span> <span class="o">*</span><span class="n">sbptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span> <span class="n">l</span><span class="o">-&gt;</span><span class="n">nodes</span><span class="p">;</span>
<span class="ln"> 5</span>    
<span class="ln"> 6</span>    <span class="n">sbptr</span> <span class="o">+=</span> <span class="n">nodeId</span> <span class="o">*</span> <span class="n">nodeSize</span><span class="p">;</span>
<span class="ln"> 7</span>    
<span class="ln"> 8</span>    <span class="k">return</span> <span class="p">(</span><span class="n">Node</span><span class="o">*</span><span class="p">)</span> <span class="n">sbptr</span><span class="p">;</span>
<span class="ln"> 9</span><span class="p">}</span>
<span class="ln">10</span>
</code></pre></div><p>Let&rsquo;s take a more detailed look at how this works. To access a particular node we simply set the pointer to the first node of this layer and then move the pointer forward by <em>the number of nodes</em> (given as the node&rsquo;s id) times <em>the size of a node</em>.
The latter is calculated by adding the default size of a node <code>sizeof(Node)</code> to the product of the number of weights (<code>wcount</code>) times the size of a weight <code>sizeof(double)</code>.</p>
<p>Accessing a particular <code>Layer</code> works similar. We place the pointer to the first layer (given by <code>nn-&gt;layers</code>) and then move it forward as required.</p>
<p>To access the <em>input</em> layer, we don&rsquo;t need to move forward at all since it <em>is</em> the first layer in the network.
To access the <em>hidden</em> layer, we need to move the pointer forward by <em>the size of the input layer</em>.
And, to access the <em>output</em> layer, we need to move the pointer forward by <em>the size of the input layer</em> plus <em>the size of the hidden layer</em>.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="n">Layer</span> <span class="o">*</span><span class="nf">getLayer</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="n">LayerType</span> <span class="n">ltype</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">l</span><span class="p">;</span>
<span class="ln"> 4</span>    
<span class="ln"> 5</span>    <span class="k">switch</span> <span class="p">(</span><span class="n">ltype</span><span class="p">)</span> <span class="p">{</span>
<span class="ln"> 6</span>        <span class="k">case</span> <span class="nl">INPUT</span><span class="p">:{</span>
<span class="ln"> 7</span>            <span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">;</span>
<span class="ln"> 8</span>            <span class="k">break</span><span class="p">;</span>
<span class="ln"> 9</span>        <span class="p">}</span>
<span class="ln">10</span>        <span class="k">case</span> <span class="nl">HIDDEN</span><span class="p">:{</span>
<span class="ln">11</span>            <span class="n">uint8_t</span> <span class="o">*</span><span class="n">sbptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">;</span>
<span class="ln">12</span>            <span class="n">sbptr</span> <span class="o">+=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">inpLayerSize</span><span class="p">;</span>
<span class="ln">13</span>            <span class="n">l</span> <span class="o">=</span> <span class="p">(</span><span class="n">Layer</span><span class="o">*</span><span class="p">)</span><span class="n">sbptr</span><span class="p">;</span>
<span class="ln">14</span>            <span class="k">break</span><span class="p">;</span>
<span class="ln">15</span>        <span class="p">}</span>
<span class="ln">16</span>            
<span class="ln">17</span>        <span class="k">default</span><span class="o">:</span><span class="p">{</span> <span class="c1">// OUTPUT
</span><span class="ln">18</span><span class="c1"></span>            <span class="n">uint8_t</span> <span class="o">*</span><span class="n">sbptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">;</span>
<span class="ln">19</span>            <span class="n">sbptr</span> <span class="o">+=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">inpLayerSize</span> <span class="o">+</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidLayerSize</span><span class="p">;</span>
<span class="ln">20</span>            <span class="n">l</span> <span class="o">=</span> <span class="p">(</span><span class="n">Layer</span><span class="o">*</span><span class="p">)</span><span class="n">sbptr</span><span class="p">;</span>
<span class="ln">21</span>            <span class="k">break</span><span class="p">;</span>
<span class="ln">22</span>        <span class="p">}</span>
<span class="ln">23</span>    <span class="p">}</span>
<span class="ln">24</span>    
<span class="ln">25</span>    <span class="k">return</span> <span class="n">l</span><span class="p">;</span>
<span class="ln">26</span><span class="p">}</span>
</code></pre></div><p>If you&rsquo;re following and understanding this post so far, the above code should be self-explanatory.
Yet, one line worth highlightening is:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="n">l</span> <span class="o">=</span> <span class="p">(</span><span class="n">Layer</span><span class="o">*</span><span class="p">)</span><span class="n">sbptr</span><span class="p">;</span>
</code></pre></div><p>I&rsquo;m using a <em>single byte pointer</em> to move through the address space but I actually need to return a <em>Layer</em> pointer as reference.
Therefore, I <em>cast</em> the <em>single byte pointer</em> into a <code>Layer</code> pointer and make it point to the same address.</p>
<h3 id="activation-function">Activation Function</h3>
<p>After the node&rsquo;s output value has been calculated we need to pass this value through an <em>activation function</em>.
The purpose of the <em>activation function</em> is to constrain the output value (which could be very large) to a standard range, for example, to between 0 and +1, or to between -1 and +1.</p>
<p>The exact range depends on <em>what</em> activation function you use. The one most often refered to in the context of designing neural networks is the SIGMOID function.
I&rsquo;m not going into the mathematical details of this function as this is outside of the scope of this post.
If you&rsquo;re interested, there are plenty of good explanations about SIGMOID and other <em>activation functions</em> on the web.</p>
<p>Since I want the network to be flexible as to what <em>activation function</em> it uses I decided to make this a parameter of the network.
The current code suppports two in particular, SIGMOID and TANH, but this list could be expanded as needed.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="k">typedef</span> <span class="k">enum</span> <span class="n">ActFctType</span> <span class="p">{</span><span class="n">SIGMOID</span><span class="p">,</span> <span class="n">TANH</span><span class="p">}</span> <span class="n">ActFctType</span><span class="p">;</span>
</code></pre></div><p>What type of <em>activation function</em> is desired can be defined after the network has been created.
It is also possible to choose a different <em>activation function</em> for hidden and output layer.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span>    <span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidLayerActType</span> <span class="o">=</span> <span class="n">SIGMOID</span><span class="p">;</span>
<span class="ln">2</span>    <span class="n">nn</span><span class="o">-&gt;</span><span class="n">outLayerActType</span> <span class="o">=</span> <span class="n">SIGMOID</span><span class="p">;</span>
</code></pre></div><p>The reason why I made this a network parameter instead of a function argument passed into the <code>activation function</code> is that the back-propagation algorithm (see below) requires using the derivative of this function.
So we need to remember how we <em>activated</em> a node because when we later adjust its weights the derivate of the same function must be used.</p>
<p>The different types of <em>activation function</em> the network supports are implement in below <code>activateNode()</code> function which applies what has been defined in <code>nn-&gt;hidLayerActType</code> and <code>nn-&gt;outLayerActType</code> variables to the hidden and ouput layer respectively:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="kt">void</span> <span class="nf">activateNode</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="n">LayerType</span> <span class="n">ltype</span><span class="p">,</span> <span class="kt">int</span> <span class="n">id</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">l</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">ltype</span><span class="p">);</span>
<span class="ln"> 4</span>    <span class="n">Node</span> <span class="o">*</span><span class="n">n</span> <span class="o">=</span> <span class="n">getNode</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">id</span><span class="p">);</span>
<span class="ln"> 5</span>    
<span class="ln"> 6</span>    <span class="n">ActFctType</span> <span class="n">actFct</span><span class="p">;</span>
<span class="ln"> 7</span>    
<span class="ln"> 8</span>    <span class="k">if</span> <span class="p">(</span><span class="n">ltype</span><span class="o">==</span><span class="n">HIDDEN</span><span class="p">)</span> <span class="n">actFct</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidLayerActType</span><span class="p">;</span>
<span class="ln"> 9</span>    <span class="k">else</span> <span class="n">actFct</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">outLayerActType</span><span class="p">;</span>
<span class="ln">10</span>    
<span class="ln">11</span>    <span class="k">if</span> <span class="p">(</span><span class="n">actFct</span><span class="o">==</span><span class="n">TANH</span><span class="p">)</span>   <span class="n">n</span><span class="o">-&gt;</span><span class="n">output</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">n</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">);</span>
<span class="ln">12</span>    <span class="k">else</span> <span class="n">n</span><span class="o">-&gt;</span><span class="n">output</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">exp</span><span class="p">((</span><span class="kt">double</span><span class="p">)</span><span class="o">-</span><span class="n">n</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">))</span> <span class="p">);</span>
<span class="ln">13</span>    
<span class="ln">14</span><span class="p">}</span>
</code></pre></div><h3 id="3-feed-backward-error-back-propagation">3. Feed Backward (Error Back-Propagation)</h3>
<p>After we finished our <em>feed forward</em> we start to <em>back propagate</em> the <em>error</em>.
The <em>error</em> here refers to the difference of the <em>desired</em> or <em>target</em> output (which in MNIST is provided as the image&rsquo;s <em>label</em>) and the <em>actual</em> output of the output layer nodes.</p>
<p>Let&rsquo;s do this step by step, or layer by layer:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="kt">void</span> <span class="nf">backPropagateNetwork</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="kt">int</span> <span class="n">targetClassification</span><span class="p">){</span>
<span class="ln">2</span>    
<span class="ln">3</span>    <span class="n">backPropagateOutputLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">targetClassification</span><span class="p">);</span>
<span class="ln">4</span>    
<span class="ln">5</span>    <span class="n">backPropagateHiddenLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">targetClassification</span><span class="p">);</span> 
<span class="ln">6</span><span class="p">}</span>
</code></pre></div><p>We first back propagate the output layer:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="kt">void</span> <span class="nf">backPropagateOutputLayer</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="kt">int</span> <span class="n">targetClassification</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">ol</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">OUTPUT</span><span class="p">);</span>
<span class="ln"> 4</span>    
<span class="ln"> 5</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">o</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">o</span><span class="o">&lt;</span><span class="n">ol</span><span class="o">-&gt;</span><span class="n">ncount</span><span class="p">;</span><span class="n">o</span><span class="o">++</span><span class="p">){</span>
<span class="ln"> 6</span>        
<span class="ln"> 7</span>        <span class="n">Node</span> <span class="o">*</span><span class="n">on</span> <span class="o">=</span> <span class="n">getNode</span><span class="p">(</span><span class="n">ol</span><span class="p">,</span><span class="n">o</span><span class="p">);</span>
<span class="ln"> 8</span>        
<span class="ln"> 9</span>        <span class="kt">int</span> <span class="n">targetOutput</span> <span class="o">=</span> <span class="p">(</span><span class="n">o</span><span class="o">==</span><span class="n">targetClassification</span><span class="p">)</span><span class="o">?</span><span class="mi">1</span><span class="o">:</span><span class="mi">0</span><span class="p">;</span>
<span class="ln">10</span>        
<span class="ln">11</span>        <span class="kt">double</span> <span class="n">errorDelta</span> <span class="o">=</span> <span class="n">targetOutput</span> <span class="o">-</span> <span class="n">on</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">;</span>
<span class="ln">12</span>        <span class="kt">double</span> <span class="n">errorSignal</span> <span class="o">=</span> <span class="n">errorDelta</span> <span class="o">*</span> <span class="n">getActFctDerivative</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">OUTPUT</span><span class="p">,</span> <span class="n">on</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">);</span>
<span class="ln">13</span>        
<span class="ln">14</span>        <span class="n">updateNodeWeights</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">OUTPUT</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">errorSignal</span><span class="p">);</span>
<span class="ln">15</span>        
<span class="ln">16</span>    <span class="p">}</span> 
<span class="ln">17</span><span class="p">}</span>
</code></pre></div><p>Our <code>targetClassification</code> is the image&rsquo;s <em>label</em>, i.e. a single digit 0-9.
The <code>targetOutput</code> of an output node, however, is of binary valye, i.e. either 0 or 1.</p>
<p>For example: if we train on an image presenting a &ldquo;3&rdquo;, the corresponding <em>label</em> will be the integer &ldquo;3&rdquo;.
Then the <code>targetOutput</code> of node 3 (which is actually the 4th node since we started at the 0th) will be a &ldquo;1&rdquo; while the <code>targetOutput</code> of all other 9 nodes will be &ldquo;0&rdquo;. This is done via below command:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="kt">int</span> <span class="n">targetOutput</span> <span class="o">=</span> <span class="p">(</span><span class="n">o</span><span class="o">==</span><span class="n">targetClassification</span><span class="p">)</span><span class="o">?</span><span class="mi">1</span><span class="o">:</span><span class="mi">0</span><span class="p">;</span>
</code></pre></div><p>The back propagation of the <em>hidden</em> layer works the same say, yet the actual algorithm is a little more complex since it requires to first calculate the <em>sum of all weighted outputs</em> connected to a node.
I&rsquo;m not going into the details of the back propagation algorithm in this post.
If you&rsquo;re interested, there are plenty of detailed and more qualified sources about this subject on the web.</p>
<p>In both of the above back propagation functions (for the <em>output</em> layer and for the <em>hidden</em> layer) we need to get the derivative of the activation function and update the node&rsquo;s weights.</p>
<p>The calculation of the derivates of the supported SIGMOID and TANH functions is implemented in the getActFctDerivative() function:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="kt">double</span> <span class="nf">getActFctDerivative</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="n">LayerType</span> <span class="n">ltype</span><span class="p">,</span> <span class="kt">double</span> <span class="n">outVal</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="kt">double</span> <span class="n">dVal</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln"> 4</span>    <span class="n">ActFctType</span> <span class="n">actFct</span><span class="p">;</span>
<span class="ln"> 5</span>    
<span class="ln"> 6</span>    <span class="k">if</span> <span class="p">(</span><span class="n">ltype</span><span class="o">==</span><span class="n">HIDDEN</span><span class="p">)</span> <span class="n">actFct</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidLayerActType</span><span class="p">;</span>
<span class="ln"> 7</span>                  <span class="k">else</span> <span class="n">actFct</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">outLayerActType</span><span class="p">;</span>
<span class="ln"> 8</span>    
<span class="ln"> 9</span>    <span class="k">if</span> <span class="p">(</span><span class="n">actFct</span><span class="o">==</span><span class="n">TANH</span><span class="p">)</span> <span class="n">dVal</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">pow</span><span class="p">(</span><span class="n">tanh</span><span class="p">(</span><span class="n">outVal</span><span class="p">),</span><span class="mi">2</span><span class="p">);</span>
<span class="ln">10</span>                 <span class="k">else</span> <span class="n">dVal</span> <span class="o">=</span> <span class="n">outVal</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">outVal</span><span class="p">);</span>
<span class="ln">11</span>    
<span class="ln">12</span>    <span class="k">return</span> <span class="n">dVal</span><span class="p">;</span>
<span class="ln">13</span><span class="p">}</span>
</code></pre></div><p>Lastly, each node&rsquo;s weights are updated using the back propagated error:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="kt">void</span> <span class="nf">updateNodeWeights</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">,</span> <span class="n">LayerType</span> <span class="n">ltype</span><span class="p">,</span> <span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">double</span> <span class="n">error</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">updateLayer</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">ltype</span><span class="p">);</span>
<span class="ln"> 4</span>    <span class="n">Node</span> <span class="o">*</span><span class="n">updateNode</span> <span class="o">=</span> <span class="n">getNode</span><span class="p">(</span><span class="n">updateLayer</span><span class="p">,</span> <span class="n">id</span><span class="p">);</span>
<span class="ln"> 5</span>    
<span class="ln"> 6</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">prevLayer</span><span class="p">;</span>
<span class="ln"> 7</span>    <span class="kt">int</span> <span class="n">prevLayerNodeSize</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln"> 8</span>    <span class="k">if</span> <span class="p">(</span><span class="n">ltype</span><span class="o">==</span><span class="n">HIDDEN</span><span class="p">)</span> <span class="p">{</span>
<span class="ln"> 9</span>        <span class="n">prevLayer</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">INPUT</span><span class="p">);</span>
<span class="ln">10</span>        <span class="n">prevLayerNodeSize</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">inpNodeSize</span><span class="p">;</span>
<span class="ln">11</span>    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
<span class="ln">12</span>        <span class="n">prevLayer</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">HIDDEN</span><span class="p">);</span>
<span class="ln">13</span>        <span class="n">prevLayerNodeSize</span> <span class="o">=</span> <span class="n">nn</span><span class="o">-&gt;</span><span class="n">hidNodeSize</span><span class="p">;</span>
<span class="ln">14</span>    <span class="p">}</span>
<span class="ln">15</span>    
<span class="ln">16</span>    <span class="n">uint8_t</span> <span class="o">*</span><span class="n">sbptr</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint8_t</span><span class="o">*</span><span class="p">)</span> <span class="n">prevLayer</span><span class="o">-&gt;</span><span class="n">nodes</span><span class="p">;</span>
<span class="ln">17</span>    
<span class="ln">18</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">updateNode</span><span class="o">-&gt;</span><span class="n">wcount</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">){</span>
<span class="ln">19</span>        <span class="n">Node</span> <span class="o">*</span><span class="n">prevLayerNode</span> <span class="o">=</span> <span class="p">(</span><span class="n">Node</span><span class="o">*</span><span class="p">)</span><span class="n">sbptr</span><span class="p">;</span>
<span class="ln">20</span>        <span class="n">updateNode</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">nn</span><span class="o">-&gt;</span><span class="n">learningRate</span> <span class="o">*</span> <span class="n">prevLayerNode</span><span class="o">-&gt;</span><span class="n">output</span> <span class="o">*</span> <span class="n">error</span><span class="p">);</span>
<span class="ln">21</span>        <span class="n">sbptr</span> <span class="o">+=</span> <span class="n">prevLayerNodeSize</span><span class="p">;</span>
<span class="ln">22</span>    <span class="p">}</span>
<span class="ln">23</span>    
<span class="ln">24</span>    <span class="c1">// update bias weight
</span><span class="ln">25</span><span class="c1"></span>    <span class="n">updateNode</span><span class="o">-&gt;</span><span class="n">bias</span> <span class="o">+=</span> <span class="p">(</span><span class="n">nn</span><span class="o">-&gt;</span><span class="n">learningRate</span> <span class="o">*</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">error</span><span class="p">);</span>
<span class="ln">26</span>    
<span class="ln">27</span><span class="p">}</span>
</code></pre></div><p>On a side note, I decided to make the <em>learning rate</em> part of the network (instead of a <em>global</em> constant).
It can be set as follows:</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span><span class="n">nn</span><span class="o">-&gt;</span><span class="n">learningRate</span>    <span class="o">=</span> <span class="mf">0.5</span><span class="p">;</span>
</code></pre></div><h3 id="4-classify-output">4. Classify Output</h3>
<p>Now that the error has been back propagated and the weights have been updated, we can query our network to attempt to classify the image. This is simply done by retrieving the index of the output node with the highest value.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln"> 1</span><span class="kt">int</span> <span class="nf">getNetworkClassification</span><span class="p">(</span><span class="n">Network</span> <span class="o">*</span><span class="n">nn</span><span class="p">){</span>
<span class="ln"> 2</span>    
<span class="ln"> 3</span>    <span class="n">Layer</span> <span class="o">*</span><span class="n">l</span> <span class="o">=</span> <span class="n">getLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">OUTPUT</span><span class="p">);</span>
<span class="ln"> 4</span>    
<span class="ln"> 5</span>    <span class="kt">double</span> <span class="n">maxOut</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln"> 6</span>    <span class="kt">int</span> <span class="n">maxInd</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="ln"> 7</span>    
<span class="ln"> 8</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">l</span><span class="o">-&gt;</span><span class="n">ncount</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">){</span>
<span class="ln"> 9</span>        
<span class="ln">10</span>        <span class="n">Node</span> <span class="o">*</span><span class="n">on</span> <span class="o">=</span> <span class="n">getNode</span><span class="p">(</span><span class="n">l</span><span class="p">,</span><span class="n">i</span><span class="p">);</span>
<span class="ln">11</span>        
<span class="ln">12</span>        <span class="k">if</span> <span class="p">(</span><span class="n">on</span><span class="o">-&gt;</span><span class="n">output</span> <span class="o">&gt;</span> <span class="n">maxOut</span><span class="p">){</span>
<span class="ln">13</span>            <span class="n">maxOut</span> <span class="o">=</span> <span class="n">on</span><span class="o">-&gt;</span><span class="n">output</span><span class="p">;</span>
<span class="ln">14</span>            <span class="n">maxInd</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
<span class="ln">15</span>        <span class="p">}</span>
<span class="ln">16</span>    <span class="p">}</span>
<span class="ln">17</span>    
<span class="ln">18</span>    <span class="k">return</span> <span class="n">maxInd</span><span class="p">;</span>
<span class="ln">19</span><span class="p">}</span>
</code></pre></div><p>That&rsquo;s it. We&rsquo;re done. So I thought. Only to discover that this is when the <em>other</em>, less obvious part of building a neural network starts.</p>
<h2 id="fine-tuning-network-performance">Fine-tuning Network Performance</h2>
<p>The neural network&rsquo;s accuracy is defined as the ratio of correct classifications (<em>in the testing set</em>) to the total number of images processed.</p>
<p>Using the code above, my 3-layer network achieves an out-of-the-box accuracy of (<em>only</em>) 91% which is slightly better than the 85% of the simple 1-layer network I built before. How come the improvement is so <em>little</em>?</p>
<p>This post so far may give the impression that building and coding a neural network is a pretty straight forward and deterministic excercise.
Unfortunately, it&rsquo;s not.</p>
<p>There are a number of parameters that may strongly impact the network&rsquo;s performance.
Sometimes even a slight change dramatically impacts the network&rsquo;s accuracy which during the coding process for this post often led me to believe that my algorithm and/or code were wrong while they were not. It was just wrongly set parameters.</p>
<p>In particular, I found the following parameters to most significantly impact the network&rsquo;s performance:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback"><span class="ln">1</span>1. Number of nodes in the hidden layer
<span class="ln">2</span>2. Type of activation function
<span class="ln">3</span>3. Values of initial weights
<span class="ln">4</span>4. Learning rate
<span class="ln">5</span>
</code></pre></div><p>Let&rsquo;s go through them.</p>
<h3 id="number-of-nodes-in-the-hidden-layer">Number of Nodes in the Hidden Layer</h3>
<p>This one is obvious. The higher the number of hidden nodes the more the network will adapt to the training data and &ldquo;remember&rdquo; it, thereby preventing <em>generalization</em>.
But <em>generalization</em>, i.e. the ability to apply features that have been <em>learned</em> during training to completely new and unknown data sets, is exactly what we do want from our network.</p>
<p>On the other side, the smaller the number of nodes in the hidden layer, the less complexity the network will be able to grasp. For example, in our MNIST database, there may be many different ways of hand-writing a &ldquo;4&rdquo;. The more complex our data set, the more hidden nodes are needed.</p>
<p>Via numerous trials I found that using 20 nodes in the hidden layer achieves the best result.</p>
<h3 id="activation-function-1">Activation Function</h3>
<p>This one is also obvious, albeit a little less so.
I found that changing the <em>activation function</em> requires to make additional parameters change as well to avoid a significant performance hit.
In particular, the <em>activation function</em> is linked to what initial weights were chosen (see below) and to what learning rate was defined (see below).</p>
<p>In my tests I found that SIGMOID performed slightly better than TANH, although I suppose this is rather due to my overall design. Based on my reading I had expected that TANH outperforms SIGMOID.</p>
<h3 id="initial-weights">Initial Weights</h3>
<p>This one is a lot less obvious. The weights in neural networks are usually initialized using random values 0 to +1 or -1 to +1.
In fact, in my previous 1-layer MNIST network I had found that initializing all weights to 0.5 leads to almost the same result as initializing them to a random value between 0 and +1.</p>
<p>Now, with the added hidden layer and the use of an activation function, the network behaves very differently.</p>
<p>I started by using random values between 0 and +1 and wondered about the desastrous performance.
I then changed to include negative values, i.e. using random values -1 to +1 which improved performance a lot.
If you understand how SIGMOID and TANH work, though, it&rsquo;s less of a surprise why this subtle change has a rather large impact.</p>
<p>Yet, while I was trying to further fine-tune the network, I found that another subtle change significantly improved the network&rsquo;s accuracy:
instead of using random values between -1 and +1 I used values between 0 and +1 and made every second value negative.</p>
<div class="highlight"><pre class="chroma"><code class="language-c" data-lang="c"><span class="ln">1</span>    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">o</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">o</span><span class="o">&lt;</span><span class="n">weightCount</span><span class="p">;</span><span class="n">o</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
<span class="ln">2</span>        <span class="n">dn</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">[</span><span class="n">o</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">rand</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="kt">double</span><span class="p">)(</span><span class="n">RAND_MAX</span><span class="p">)));</span>
<span class="ln">3</span>        <span class="k">if</span> <span class="p">(</span><span class="n">o</span><span class="o">%</span><span class="mi">2</span><span class="p">)</span> <span class="n">dn</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">[</span><span class="n">o</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">dn</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">[</span><span class="n">o</span><span class="p">];</span>  <span class="c1">// make half of the numbers negative
</span><span class="ln">4</span><span class="c1"></span>    <span class="p">}</span>
</code></pre></div><p>Wow! This was unexpected. It led me to the conclusion that for best performance the network needs to be initialized not only randomly but <em>evenly</em> random or <em>symetrically</em> random.</p>
<h3 id="learning-rate">Learning Rate</h3>
<p>While this parameter is obvious, it&rsquo;s significant impact on the network&rsquo;s performance did surprise me several times.</p>
<p>The <em>learning rate</em> defines the factor by which an <em>error</em> is applied as a <em>weight update</em>.
The higher the <em>learning rate</em> the faster the network adapts, i.e. the faster it should reach its point of optimal performance.
However, if the rate is too high the network may <em>jump</em> over the optimum and may settle in a point of lower accuracy.</p>
<p>On the other side, a smaller <em>learning rate</em> allows the network to make very fine changes but overall it may take so long that it reaches the end of the dataset before reaching its optimal point of performance.</p>
<p>In my tests I found that the <em>optimal</em> learning rate depends on the chosen type of activation function.
While for SIGMOID my network did well using a learning rate of 0.2 (91% accuracy), I had to change it to 0.004 to achieve highest performance with TANH (78% accuracy).</p>
<h2 id="conclusion">Conclusion</h2>
<p>Coding this 3-layer neural network to recognize the MNIST digits has been an interesting excercise.
It provided valuable insights into the <em>unpredictability</em> of neural networks.
In particular, it helped to alert me to how small parameter changes can cause very different results.</p>
<p>Overall, the network&rsquo;s performance, i.e. its accuracy in recognizing the MNIST digits, is still disappointing.
Further fine-tuning is required, e.g. using a dynamic learning rate.
I experimented with different parameters and algorithm changes which helped pushing accuracy further into the 9x% (but did not implement them in the published code).</p>
<p>Next, instead of spending much more time on fine tuning this rather simple feed forward network to further improve its performance on MNIST I&rsquo;d rather want to move on to using a convolutional network. ;)</p>
<hr>
<h2 id="code--documentation">Code &amp; Documentation</h2>
<p>You can find all the code for this exercise on my <a href="https://github.com/mmlind/mnist-3lnn/">Github project page</a>, including full <a href="https://rawgit.com/mmlind/mnist-3lnn/master/doc/html/index.html">code documentation</a>.</p>
<p>When I run it on my 2010 MacBook Pro, using 784 input nodes, 20 hidden nodes and 10 output nodes, it takes about 19 seconds to process all 70,000 images (with the image rendering turned-off) and achieves an accuracy on the <em>testing set</em> of 91.5%.</p>
<p>Happy Hacking!</p>
<p><img src="/images/mnist_logo.png" alt=""></p>

    <script src="https://utteranc.es/client.js"
        repo="mmlind/mmlind.github.io"
        issue-term="pathname"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>

  </article>
<aside class="sidebar">
  <section class="sidebar_inner">
    <h2>Matt Lind</h2>
    <div>
      CTO, freelancer, and passionate software engineer. Azure cloud and M365 global admin. Excited about parellel processing and distributed systems. AI enthusiast with special interest in natural language processing (NLP)...
    </div>
    <a href = 'https://mmlind.github.io/about/' class="button mt-1" role="button">Read More</a>
    <h2 class="mt-4">Featured Posts</h2>
    <ul>
    
      <li>
        <a href="https://mmlind.github.io/posts/how_to_synchronize_multiprocessing_access_to_shared_memory/" class="nav-link">How to simultaneously write to shared memory with multiple processes</a>
      </li>
    
    </ul>
    <h2 class="mt-4">Recent Posts</h2>
    <ul class="flex-column">
      
      <li>
        <a href="https://mmlind.github.io/posts/how_to_synchronize_multiprocessing_access_to_shared_memory/" class="nav-link">How to simultaneously write to shared memory with multiple processes</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/migrating_blog_from_jekyl_hugo/" class="nav-link">Migrating my GitHub pages blog from Jekyl to Hugo</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/using_logistic_regression_to_solve_mnist/" class="nav-link">Using logistic regression to classify images</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/linear_regression/" class="nav-link">Understanding linear regression</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/deep_neural_network_for_mnist_handwriting_recognition/" class="nav-link">Deep neural network for MNIST handwriting recognition</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/simple_1-layer_neural_network_for_mnist_handwriting_recognition/" class="nav-link">Simple 1-layer neural network for MNIST handwriting recognition</a>
      </li>
      <li>
        <a href="https://mmlind.github.io/posts/what_is_a_neural_network/" class="nav-link">What is a neural network?</a>
      </li>
    </ul> 
    
    
    
    
    
    
    <div>
      <h2 class="mt-4 taxonomy" id="tags-section">Tags</h2>
      <nav class="tags_nav">
        <a href='https://mmlind.github.io/tags/machine-learning/' class=" post_tag button button_translucent">
          MACHINE-LEARNING
          <span class='button_tally'>6</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/computer-vision/' class=" post_tag button button_translucent">
          COMPUTER-VISION
          <span class='button_tally'>4</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/blogging/' class=" post_tag button button_translucent">
          BLOGGING
          <span class='button_tally'>2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/math/' class=" post_tag button button_translucent">
          MATH
          <span class='button_tally'>2</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/c/' class=" post_tag button button_translucent">
          C
          <span class='button_tally'>1</span>
        </a>
        
        <a href='https://mmlind.github.io/tags/multiprocessing/' class=" post_tag button button_translucent">
          MULTIPROCESSING
          <span class='button_tally'>1</span>
        </a>
        
        
      </nav>
    </div>
    
    
  </section>
</aside>

</div>
    </main><svg width="0" height="0" class="hidden">
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="facebook">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h151V331h-60v-90h60v-61c0-49.629 40.371-90 90-90h91v90h-91v61h91l-15 90h-76v181h121c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18.001 18.001" id="twitter">
    <path d="M15.891 4.013c.808-.496 1.343-1.173 1.605-2.034a8.68 8.68 0 0 1-2.351.861c-.703-.756-1.593-1.14-2.66-1.14-1.043 0-1.924.366-2.643 1.078a3.56 3.56 0 0 0-1.076 2.605c0 .309.039.585.117.819-3.076-.105-5.622-1.381-7.628-3.837-.34.601-.51 1.213-.51 1.846 0 1.301.549 2.332 1.645 3.089-.625-.053-1.176-.211-1.645-.47 0 .929.273 1.705.82 2.388a3.623 3.623 0 0 0 2.115 1.291c-.312.08-.641.118-.979.118-.312 0-.533-.026-.664-.083.23.757.664 1.371 1.291 1.841a3.652 3.652 0 0 0 2.152.743C4.148 14.173 2.625 14.69.902 14.69c-.422 0-.721-.006-.902-.038 1.697 1.102 3.586 1.649 5.676 1.649 2.139 0 4.029-.542 5.674-1.626 1.645-1.078 2.859-2.408 3.639-3.974a10.77 10.77 0 0 0 1.172-4.892v-.468a7.788 7.788 0 0 0 1.84-1.921 8.142 8.142 0 0 1-2.11.593z"
      ></path>
  </symbol>
  <symbol aria-hidden="true" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="mail">
    <path  d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="calendar">
    <path d="M452 40h-24V0h-40v40H124V0H84v40H60C26.916 40 0 66.916 0 100v352c0 33.084 26.916 60 60 60h392c33.084 0 60-26.916 60-60V100c0-33.084-26.916-60-60-60zm20 412c0 11.028-8.972 20-20 20H60c-11.028 0-20-8.972-20-20V188h432v264zm0-304H40v-48c0-11.028 8.972-20 20-20h24v40h40V80h264v40h40V80h24c11.028 0 20 8.972 20 20v48z"></path>
    <path d="M76 230h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 310h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zM76 390h40v40H76zm80 0h40v40h-40zm80 0h40v40h-40zm80 0h40v40h-40zm80-80h40v40h-40z"></path>
  </symbol>
  <symbol viewBox="-21 0 512 512" xmlns="http://www.w3.org/2000/svg" id="yank">
    <path d="M410.668 405.332H165.332c-32.363 0-58.664-26.3-58.664-58.664v-288c0-32.363 26.3-58.668 58.664-58.668h181.504c21.059 0 41.687 8.535 56.555 23.445l42.496 42.496c15.125 15.125 23.445 35.223 23.445 56.575v224.152c0 32.363-26.3 58.664-58.664 58.664zM165.332 32c-14.7 0-26.664 11.969-26.664 26.668v288c0 14.7 11.965 26.664 26.664 26.664h245.336c14.7 0 26.664-11.965 26.664-26.664V122.516c0-12.82-4.992-24.871-14.059-33.942l-42.496-42.496C371.84 37.121 359.488 32 346.836 32zm0 0"></path>
    <path d="M314.668 512h-256C26.305 512 0 485.695 0 453.332V112c0-32.363 26.305-58.668 58.668-58.668h10.664c8.832 0 16 7.168 16 16s-7.168 16-16 16H58.668C43.968 85.332 32 97.301 32 112v341.332C32 468.032 43.969 480 58.668 480h256c14.7 0 26.664-11.969 26.664-26.668v-10.664c0-8.832 7.168-16 16-16s16 7.168 16 16v10.664c0 32.363-26.3 58.668-58.664 58.668zM368 181.332H208c-8.832 0-16-7.168-16-16s7.168-16 16-16h160c8.832 0 16 7.168 16 16s-7.168 16-16 16zm0 0"></path>
    <path d="M368 245.332H208c-8.832 0-16-7.168-16-16s7.168-16 16-16h160c8.832 0 16 7.168 16 16s-7.168 16-16 16zm0 64H208c-8.832 0-16-7.168-16-16s7.168-16 16-16h160c8.832 0 16 7.168 16 16s-7.168 16-16 16zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="github">
    <path d="M255.968 5.329C114.624 5.329 0 120.401 0 262.353c0 113.536 73.344 209.856 175.104 243.872 12.8 2.368 17.472-5.568 17.472-12.384 0-6.112-.224-22.272-.352-43.712-71.2 15.52-86.24-34.464-86.24-34.464-11.616-29.696-28.416-37.6-28.416-37.6-23.264-15.936 1.728-15.616 1.728-15.616 25.696 1.824 39.2 26.496 39.2 26.496 22.848 39.264 59.936 27.936 74.528 21.344 2.304-16.608 8.928-27.936 16.256-34.368-56.832-6.496-116.608-28.544-116.608-127.008 0-28.064 9.984-51.008 26.368-68.992-2.656-6.496-11.424-32.64 2.496-68 0 0 21.504-6.912 70.4 26.336 20.416-5.696 42.304-8.544 64.096-8.64 21.728.128 43.648 2.944 64.096 8.672 48.864-33.248 70.336-26.336 70.336-26.336 13.952 35.392 5.184 61.504 2.56 68 16.416 17.984 26.304 40.928 26.304 68.992 0 98.72-59.84 120.448-116.864 126.816 9.184 7.936 17.376 23.616 17.376 47.584 0 34.368-.32 62.08-.32 70.496 0 6.88 4.608 14.88 17.6 12.352C438.72 472.145 512 375.857 512 262.353 512 120.401 397.376 5.329 255.968 5.329z"></path>
  </symbol>
  <symbol viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" id="rss">
    <circle cx="3.429" cy="20.571" r="3.429"></circle>
    <path d="M11.429 24h4.57C15.999 15.179 8.821 8.001 0 8v4.572c6.302.001 11.429 5.126 11.429 11.428z"></path>
    <path d="M24 24C24 10.766 13.234 0 0 0v4.571c10.714 0 19.43 8.714 19.43 19.429z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="linkedin">
    <path d="M437 0H75C33.648 0 0 33.648 0 75v362c0 41.352 33.648 75 75 75h362c41.352 0 75-33.648 75-75V75c0-41.352-33.648-75-75-75zM181 406h-60V196h60zm0-240h-60v-60h60zm210 240h-60V286c0-16.54-13.46-30-30-30s-30 13.46-30 30v120h-60V196h60v11.309C286.719 202.422 296.93 196 316 196c40.691.043 75 36.547 75 79.688zm0 0"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 612 612" id="arrow">
    <path d="M604.501 440.509L325.398 134.956c-5.331-5.357-12.423-7.627-19.386-7.27-6.989-.357-14.056 1.913-19.387 7.27L7.499 440.509c-9.999 10.024-9.999 26.298 0 36.323s26.223 10.024 36.222 0l262.293-287.164L568.28 476.832c9.999 10.024 26.222 10.024 36.221 0 9.999-10.023 9.999-26.298 0-36.323z"></path>
  </symbol>
  <symbol viewBox="0 0 512 512" xmlns="http://www.w3.org/2000/svg" id="carly">
    <path d="M504.971 239.029L448 182.059V84c0-46.317-37.682-84-84-84h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c19.851 0 36 16.149 36 36v108c0 6.365 2.529 12.47 7.029 16.971L454.059 256l-47.029 47.029A24.002 24.002 0 0 0 400 320v108c0 19.851-16.149 36-36 36h-44c-13.255 0-24 10.745-24 24s10.745 24 24 24h44c46.318 0 84-37.683 84-84v-98.059l56.971-56.971c9.372-9.372 9.372-24.568 0-33.941zM112 192V84c0-19.851 16.149-36 36-36h44c13.255 0 24-10.745 24-24S205.255 0 192 0h-44c-46.318 0-84 37.683-84 84v98.059l-56.971 56.97c-9.373 9.373-9.373 24.568 0 33.941L64 329.941V428c0 46.317 37.682 84 84 84h44c13.255 0 24-10.745 24-24s-10.745-24-24-24h-44c-19.851 0-36-16.149-36-36V320c0-6.365-2.529-12.47-7.029-16.971L57.941 256l47.029-47.029A24.002 24.002 0 0 0 112 192z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="copy">
    <path d="M366.345 406.069H66.207c-9.751 0-17.655-7.904-17.655-17.655V17.655C48.552 7.904 56.456 0 66.207 0h300.138C376.096 0 384 7.904 384 17.655v370.759c0 9.751-7.904 17.655-17.655 17.655z" fill="#f1f4fb"></path>
    <path d="M384 388.414v-76.992c-.907.322-1.869.494-2.751.882-6.307-6.593-14.733-11.025-24.111-11.964a40.49 40.49 0 0 0-8.448.039v-92.93c0-21.903-17.82-39.723-39.723-39.724a40.5 40.5 0 0 0-4.036.202c-20.012 2.004-35.689 19.916-35.689 40.781v101.773l-21.581 21.581c-15.241 15.241-21.393 36.866-16.456 57.847l3.802 16.16h131.338c9.75 0 17.655-7.905 17.655-17.655z"
      fill="#d5dced"></path>
    <circle cx="128" cy="105.931" r="26.483" fill="#b4e66e"></circle>
    <circle cx="128" cy="203.034" r="26.483" fill="#dae169"></circle>
    <circle cx="128" cy="300.138" r="26.483" fill="#ffdc64"></circle>
    <path d="M331.034 229.517H189.793c-4.879 0-8.828-3.953-8.828-8.828s3.948-8.828 8.828-8.828h141.241c4.879 0 8.828 3.953 8.828 8.828s-3.948 8.828-8.828 8.828z" fill="#7f8499"></path>
    <path d="M295.724 194.207H189.793a8.826 8.826 0 0 1-8.828-8.828 8.826 8.826 0 0 1 8.828-8.828h105.931a8.826 8.826 0 0 1 8.828 8.828 8.826 8.826 0 0 1-8.828 8.828z" fill="#5b5d6e"></path>
    <path d="M331.034 326.621H189.793c-4.879 0-8.828-3.953-8.828-8.828s3.948-8.828 8.828-8.828h141.241c4.879 0 8.828 3.953 8.828 8.828s-3.948 8.828-8.828 8.828z" fill="#7f8499"></path>
    <path d="M295.724 291.31H189.793c-4.879 0-8.828-3.953-8.828-8.828s3.948-8.828 8.828-8.828h105.931c4.879 0 8.828 3.953 8.828 8.828s-3.948 8.828-8.828 8.828z" fill="#5b5d6e"></path>
    <path d="M331.034 132.414H189.793a8.826 8.826 0 0 1-8.828-8.828 8.826 8.826 0 0 1 8.828-8.828h141.241a8.826 8.826 0 0 1 8.828 8.828 8.826 8.826 0 0 1-8.828 8.828z" fill="#7f8499"></path>
    <path d="M295.724 97.103H189.793a8.826 8.826 0 0 1-8.828-8.828 8.826 8.826 0 0 1 8.828-8.828h105.931a8.826 8.826 0 0 1 8.828 8.828 8.825 8.825 0 0 1-8.828 8.828z" fill="#5b5d6e"></path>
    <path d="M443.656 335.563c-13.21-1.323-24.345 9.015-24.345 21.954v-7.569c0-11.544-8.306-22.063-19.794-23.213-13.209-1.323-24.344 9.015-24.344 21.954v-7.569c0-11.544-8.306-22.063-19.794-23.213-13.209-1.323-24.344 9.015-24.344 21.954V207.448c0-12.939-11.135-23.277-24.345-21.954-11.486 1.15-19.793 11.669-19.793 23.213v109.086l-26.752 26.752a44.14 44.14 0 0 0-11.754 41.32l18.47 78.495c6.567 27.913 31.475 47.64 60.15 47.64h74.645c34.127 0 61.793-27.666 61.793-61.793v-91.431c-.001-11.544-8.306-22.063-19.793-23.213z"
      fill="#f0c087"></path>
    <path d="M339.862 361.377a8.829 8.829 0 0 0 8.828-8.828v-34.194c-10.052 2.061-17.655 10.844-17.655 21.506v12.687a8.827 8.827 0 0 0 8.827 8.829zM384 370.205a8.829 8.829 0 0 0 8.828-8.828v-34.194c-10.052 2.061-17.655 10.844-17.655 21.506v12.687a8.827 8.827 0 0 0 8.827 8.829zm44.138 8.827a8.829 8.829 0 0 0 8.828-8.828V336.01c-10.052 2.061-17.655 10.844-17.655 21.506v12.687a8.827 8.827 0 0 0 8.827 8.829zM288.885 464.36l-20.467-86.985a28.482 28.482 0 0 1 7.585-26.663l10.893-10.894v22.113a8.829 8.829 0 0 0 17.656 0V185.933c-10.344 2.173-17.655 11.972-17.655 22.773v109.087l-26.752 26.752a44.14 44.14 0 0 0-11.754 41.32l18.47 78.495c6.567 27.913 31.475 47.64 60.15 47.64h22.026c-28.677 0-53.584-19.727-60.152-47.64z"
      fill="#e6af78"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512.001 512.001" id="closeme">
    <path d="M284.286 256.002L506.143 34.144c7.811-7.811 7.811-20.475 0-28.285-7.811-7.81-20.475-7.811-28.285 0L256 227.717 34.143 5.859c-7.811-7.811-20.475-7.811-28.285 0-7.81 7.811-7.811 20.475 0 28.285l221.857 221.857L5.858 477.859c-7.811 7.811-7.811 20.475 0 28.285a19.938 19.938 0 0 0 14.143 5.857 19.94 19.94 0 0 0 14.143-5.857L256 284.287l221.857 221.857c3.905 3.905 9.024 5.857 14.143 5.857s10.237-1.952 14.143-5.857c7.811-7.811 7.811-20.475 0-28.285L284.286 256.002z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" id="open-menu">
    <path d="M492 236H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0-160H20C8.954 76 0 84.954 0 96s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20zm0 320H20c-11.046 0-20 8.954-20 20s8.954 20 20 20h472c11.046 0 20-8.954 20-20s-8.954-20-20-20z"></path>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id="instagram">
    <path d="M12 2.163c3.204 0 3.584.012 4.85.07 3.252.148 4.771 1.691 4.919 4.919.058 1.265.069 1.645.069 4.849 0 3.205-.012 3.584-.069 4.849-.149 3.225-1.664 4.771-4.919 4.919-1.266.058-1.644.07-4.85.07-3.204 0-3.584-.012-4.849-.07-3.26-.149-4.771-1.699-4.919-4.92-.058-1.265-.07-1.644-.07-4.849 0-3.204.013-3.583.07-4.849.149-3.227 1.664-4.771 4.919-4.919 1.266-.057 1.645-.069 4.849-.069zm0-2.163c-3.259 0-3.667.014-4.947.072-4.358.2-6.78 2.618-6.98 6.98-.059 1.281-.073 1.689-.073 4.948 0 3.259.014 3.668.072 4.948.2 4.358 2.618 6.78 6.98 6.98 1.281.058 1.689.072 4.948.072 3.259 0 3.668-.014 4.948-.072 4.354-.2 6.782-2.618 6.979-6.98.059-1.28.073-1.689.073-4.948 0-3.259-.014-3.667-.072-4.947-.196-4.354-2.617-6.78-6.979-6.98-1.281-.059-1.69-.073-4.949-.073zm0 5.838c-3.403 0-6.162 2.759-6.162 6.162s2.759 6.163 6.162 6.163 6.162-2.759 6.162-6.163c0-3.403-2.759-6.162-6.162-6.162zm0 10.162c-2.209 0-4-1.79-4-4 0-2.209 1.791-4 4-4s4 1.791 4 4c0 2.21-1.791 4-4 4zm6.406-11.845c-.796 0-1.441.645-1.441 1.44s.645 1.44 1.441 1.44c.795 0 1.439-.645 1.439-1.44s-.644-1.44-1.439-1.44z"/>
  </symbol>
  <symbol xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" id=youtube>
    <path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z"/>
  </symbol>
</svg>
<footer class = 'footer'>
  <div class = 'footer_inner wrap pale'>
    <img src = 'https://mmlind.github.io/icons/apple-touch-icon.png' class = 'icon icon_2 transparent'>
    <p>&lt;/&gt; &copy;&nbsp;<span class = 'year'>2020</span>&nbsp;MATT&#39;S TECH BLOG. &lt;/&gt;</p>
<a class='to_top' href="#documentTop">
  <svg class="icon">
    <use xlink:href="#arrow"></use>
  </svg>
</a>

  </div>
</footer>
    <script type="text/javascript" src = "https://mmlind.github.io/js/bundle.min.7333d63e045f01aaa61cad70ba4e3060064949b5a64c00b840c4e0c664bcdb02869222e7bde3800fe4911ccad719c06a19cb5f48c60048c0e382631fbd17cece.js" integrity=
    "sha512-czPWPgRfAaqmHK1wuk4wYAZJSbWmTAC4QMTgxmS82wKGkiLnveOAD&#43;SRHMrXGcBqGctfSMYASMDjgmMfvRfOzg==" crossorigin="anonymous"></script>
  </body>
</html>
